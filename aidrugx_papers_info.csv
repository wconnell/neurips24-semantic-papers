url,title,authors,abstract,keywords,tldr,pdf_url,type
https://openreview.net/forum?id=6VldeCDKpH,Mixture of Experts Enable Efficient and Effective Protein Understanding and Design,"['Ning Sun', 'Shuxian Zou', 'Tianhua Tao', 'Sazan Mahbub', 'Dian Li', 'Yonghao Zhuang', 'Hongyi Wang', 'Xingyi Cheng', 'Le Song', 'Eric P. Xing']","Proteins play a fundamental role in life. Understanding the language of proteins
offers significant potential for gaining mechanistic insights into biological sys-
tems and introduces new avenues for treating diseases, enhancing agriculture, and
safeguarding the environment. While large protein language models (PLMs) like
ESM2-15B and xTrimoPGLM-100B have achieved remarkable performance in di-
verse protein understanding and design tasks, these models, being dense transformer
models, pose challenges due to their computational inefficiency during training
and deployment. In this work, we introduce AIDO.Protein, a pretrained module
for protein representation in an AI-driven Digital Organism [1 ]. AIDO.Protein is
also the first mixture-of-experts (MoE) model in the protein domain, with model
size scales to 16 billion parameters. Leveraging a sparse MoE architecture with
8 experts within each transformer block and selectively activating 2 experts for
each input token, our model is significantly more efficient in training and inference.
Through pre-training on 1.2 trillion amino acids collected from UniRef90 and
ColabfoldDB, our model achieves state-of-the-art results across most tasks in the
xTrimoPGLM benchmark. Furthermore, on over 280 ProteinGym Deep Mutational
Scanning (DMS) assays, our model achieves nearly 99% of the overall performance
of the best MSA-based model and significantly outperforms the previously reported
state-of-the-art models that do not utilize MSA. We also adapted this model for
structure-conditioned protein sequence generation tasks and achieved new SOTA
in this domain. These results indicate that AIDO.Protein can serve as a strong
foundation model for protein understanding and design. Models and codes are
available through ModelGenerator in https://github.com/genbio-ai/AIDO
and on Hugging Face.","['Protein Foundation Model', 'Sparse Experts Model', 'Protein Property Prediction', 'Protein Generation']",,https://openreview.net/pdf?id=6VldeCDKpH,spotlight
https://openreview.net/forum?id=MfphfLFhD4,Bridging the Gap between Database Search and \emph{De Novo} Peptide Sequencing with SearchNovo,"['Jun Xia', 'Sizhe Liu', 'Jingbo Zhou', 'Shaorong Chen', 'hongxin xiang', 'Zicheng Liu', 'Yue Liu', 'Stan Z. Li']","Accurate protein identification from mass spectrometry (MS) data is fundamental to unraveling the complex roles of proteins in biological systems, with peptide sequencing being a pivotal step in this process. The two main paradigms for peptide sequencing are database search, which matches experimental spectra with peptide sequences from databases, and \emph{de novo} sequencing, which infers peptide sequences directly from MS without relying on pre-constructed database. Although database search methods are highly accurate, they are limited by their inability to identify novel, modified, or mutated peptides absent from the database. In contrast, \emph{de novo} sequencing is adept at discovering novel peptides but often struggles with missing peaks issue, further leading to lower precision. We introduce SearchNovo, a novel framework that synergistically integrates the strengths of database search and \emph{de novo} sequencing to enhance peptide sequencing. SearchNovo employs an efficient search mechanism to retrieve the most similar peptide spectrum match (PSM) from a database for each query spectrum, followed by a fusion module that utilizes the reference peptide sequence to guide the generation of the target sequence. Furthermore, we observed that dissimilar (noisy) reference peptides negatively affect model performance. To mitigate this, we constructed pseudo reference PSMs to minimize their impact. Comprehensive evaluations on multiple datasets reveal that SearchNovo significantly outperforms state-of-the-art models. Also, analysis indicates that many retrieved spectra contain missing peaks absent in the query spectra, and the retrieved reference peptides often share common fragments with the target peptides. These are key elements in the recipe for SearchNovoâ€™s success. The code for reproducing the results are available in the supplementary materials.",['Peptide Sequencing'],,https://openreview.net/pdf?id=MfphfLFhD4,spotlight
https://openreview.net/forum?id=SzfRVq8X07,LLMs are Highly-Constrained Biophysical Sequence Optimizers,"['Angelica Chen', 'Samuel Don Stanton', 'Robert G Alberstein', 'Andrew Martin Watkins', 'Richard Bonneau', 'Vladimir Gligorijevic', 'Kyunghyun Cho', 'Nathan C. Frey']","Large language models (LLMs) have recently shown significant potential in various biological tasks such as protein engineering and molecule design. These tasks typically involve black-box discrete sequence optimization, where the challenge lies in generating sequences that are not only biologically feasible but also adhere to hard fine-grained constraints. However, LLMs often struggle with such constraints, especially in biological contexts where verifying candidate solutions is costly and time-consuming. In this study, we explore the possibility of employing LLMs as *highly-constrained bilevel optimizers* through a methodology we refer to as Language Model Optimization with Margin Expectation (LLOME). This approach combines both offline and online optimization, utilizing limited oracle evaluations to iteratively enhance the sequences generated by the LLM. We additionally propose a novel training objective -- Margin-Aligned Expectation (MargE) -- that trains the LLM to smoothly interpolate between the reward and reference distributions.
Lastly, we introduce a synthetic test suite that bears strong geometric similarity to real biophysical problems and enables rapid evaluation of LLM optimizers without time-consuming lab validation. Our findings reveal that, in comparison to genetic algorithm baselines, LLMs achieve significantly lower regret solutions while requiring fewer test function evaluations. However, we also observe that LLMs exhibit moderate miscalibration, are susceptible to generator collapse, and have difficulty finding the optimal solution when no explicit ground truth rewards are available.","['large language models', 'optimizers', 'biophysical sequence optimization']",,https://openreview.net/pdf?id=SzfRVq8X07,spotlight
https://openreview.net/forum?id=D8wVEAKFRA,DeepADAR: A deep learning approach to model regulatory elements of ADAR-based RNA editing and its application to gRNA design,"['Andrew J Jung', 'ALICE J. GAO', 'Leo J Lee', 'Brendan Frey']","RNA editing is an important post-transcriptional modification which increases transcriptomic diversity and regulates other cellular processes. In humans, the predominant form of RNA editing is adenosine-to-inosine (A-to-I) conversion, mediated by adenosine deaminases acting on RNA (ADAR) enzymes. Recently, RNA therapeutics leveraging endogenous ADAR to induce site-specific editing have emerged as promising approaches for correcting disease causing mutations and modulating gene expression. However, the precise mechanisms by which cis-regulatory elements control ADAR editing are highly complex and remain largely unknown. With improvements in methods for ADAR editing detection and quantification, a data-driven approach utilizing large-scale RNA-seq data from the existing literature has emerged as a promising strategy for modeling and understanding the general regulatory elements of ADAR editing. Here, we present DeepADAR, a deep learning model that learns regulatory elements of ADAR editing by training on a large number of high-confidence ADAR editing sites supported by RNA-seq data. DeepADAR not only achieves strong performance on predicting editing sites in held-out test data, but also successfully distinguishes sites with varying editing efficiencies in CRISPR mutagenesis experiments and RNA-editing quantitative trait loci (QTLs). Furthermore, we demonstrate how the endogenous regulatory elements of ADAR editing learned by DeepADAR can be utilized to predict ADAR editing induced by guide RNAs (gRNAs) in a zero-shot setting. Notably, DeepADAR can predict both on-target and off-target editing induced by gRNAs, demonstrating its potential utility in RNA therapeutics design","['RNA editing', 'RNA therapeutics', 'drug design', 'deep learning for RNA biology', 'RNA']",,https://openreview.net/pdf?id=D8wVEAKFRA,spotlight
https://openreview.net/forum?id=BMiHsF6YOd,Bayesian Optimization of Antibodies Informed by a Generative Model of Evolving Sequences,"['Alan Nawzad Amin', 'Nate Gruver', 'Yucen Lily Li', 'Yilun Kuang', 'Hunter Elliott', 'Calvin McCarter', 'Aniruddh Raghu', 'Peyton Greenside', 'Andrew Gordon Wilson']","To build effective therapeutics, biologists iteratively mutate antibody sequences to improve binding and stability. Proposed mutations can be informed by previous measurements or by learning from large antibody databases to predict only typical antibodies. Unfortunately, the space of typical antibodies is enormous to search, and experiments often fail to find suitable antibodies on a budget. Here we introduce Clone-informed Bayesian Optimization (CloneBO), a Bayesian optimization procedure that efficiently optimizes antibodies in the lab by teaching a generative model how our immune system optimizes antibodies. Our immune system makes antibodies by iteratively evolving specific portions of their sequences to bind their target strongly and stably, resulting in a set of related, evolving sequences known as a clonal family. We train a large language model, CloneLM, on hundreds of thousands of clonal families and use it to design sequences with mutations that are most likely to optimize an antibody within the human immune system. We guide our designs to fit previous measurements using a twisted sequential Monte Carlo procedure. We show that CloneBO optimizes antibodies substantially more efficiently than previous methods in realistic in silico experiments and designs stronger and more stable binders in in vitro wet lab experiments.",['Antibodies + generative models + Bayesian optimization + foundation model'],,https://openreview.net/pdf?id=BMiHsF6YOd,spotlight
https://openreview.net/forum?id=ltMdWRc7A9,Leveraging Disease-Specific Topologies and Counterfactual Relationships in Knowledge Graphs for Inductive Reasoning in Drug Repurposing,"['Cerag Oguztuzun', 'Zhenxiang Gao', 'Hui Li', 'Rong Xu']","Drug repurposing offers a cost-effective strategy to accelerate drug development by identifying new therapeutic uses for approved medications. However, it poses significant challenges for complex diseases with poorly understood mechanisms of action. Addressing these diseases requires the efficient integration of new data while minimizing retraining time, prompting us to develop domain-specific graph augmentation techniques that support semi-inductive reasoning. We discovered that leveraging counterfactual relationships derived from disease-specific topological structures significantly enhances model performance. Based on this insight, we integrated counterfactual relationships as an augmentation method and an initialization step in our knowledge graph (KG) link prediction training process. We introduce KGÃ¯A, an inductive KG augmentation method that utilizes counterfactual relationships based on disease-specific topologies. By aligning augmentation with the intrinsic topological features of disease entities, KGÃ¯A enhances the KG in a domain-specific manner, facilitating the discovery of a broader range of novel drug candidates tailored to specific diseases. Our biomedical KG comprises 1,614,801 triples and 100,563 biomedical entities, including 30,006 diseases, constructed from 6 biomedical datasets and enriched through Natural Language Processing (NLP) relation extraction. Extensive experiments on this comprehensive KG using 5 augmented architectures demonstrate that semi-inductive reasoning significantly improves generalizability (up to a 24Ã— increase in Mean Reciprocal Rank (MRR)) and that augmented models outperform state-of-the-art KG-based drug repurposing methods (up to a 32\% improvement in MRR). Additionally, in an Alzheimer's Disease (AD) case study, our model identified up to 5 mechanism categories compared to 2 in the baseline, highlighting its enhanced capability to uncover diverse drug candidates.","['drug repurposing', 'graph machine learning', 'knowledge graph', 'graph augmentation', 'inductive reasoning', 'foundation models', ""alzheimer's disease""]",,https://openreview.net/pdf?id=ltMdWRc7A9,spotlight
https://openreview.net/forum?id=Gzo3JMPY8w,A Large-Scale Foundation Model for RNA Function and Structure Prediction,"['Shuxian Zou', 'Tianhua Tao', 'Sazan Mahbub', 'Caleb Ellington', 'Robin Jonathan Algayres', 'Dian Li', 'Yonghao Zhuang', 'Hongyi Wang', 'Le Song', 'Eric P. Xing']","Originally marginalized as an intermediate in the information flow from DNA to
protein, RNA has become the star of modern biology, holding the key to precision
therapeutics, genetic engineering, evolutionary origins, and our understanding of
fundamental cellular processes. Yet RNA is as mysterious as it is prolific, serving
as an information store, a messenger, and a catalyst, spanning many undercharacterized
functional and structural classes. Deciphering the language of RNA is
important not only for a mechanistic understanding of its biological functions but
also for accelerating drug design. Toward this goal, we introduce AIDO.RNA, a
pre-trained module for RNA in an AI-driven Digital Organism [1]. AIDO.RNA
contains a scale of 1.6 billion parameters, trained on 42 million non-coding RNA
(ncRNA) sequences at single-nucleotide resolution, and it achieves state-of-theart
performance on a comprehensive set of tasks, including structure prediction,
genetic regulation, molecular function across species, and RNA sequence design.
AIDO.RNA after domain adaptation learns to model essential parts of protein translation
that protein language models, which have received widespread attention in
recent years, do not. More broadly, AIDO.RNA hints at the generality of biological
sequence modeling and the ability to leverage the central dogma to improve many
biomolecular representations. Models and code are available through ModelGenerator
in https://github.com/genbio-ai/AIDO and on Hugging Face.","['Foundation model', 'RNA', 'Drug design']",,https://openreview.net/pdf?id=Gzo3JMPY8w,spotlight
https://openreview.net/forum?id=1EJXtK3AWr,PerturBench: Benchmarking Machine Learning Models for Cellular Perturbation Analysis,"['Yan Wu', 'Esther Wershof', 'Sebastian M Schmon', 'Marcel Nassar', 'BÅ‚aÅ¼ej OsiÅ„ski', 'Ridvan Eksi', 'Kun Zhang', 'Thore Graepel']","We present a comprehensive framework for predicting the effects of perturbations in single cells, designed to standardize benchmarking in this rapidly evolving field. Our framework, PerturBench, includes a user-friendly platform, diverse datasets, metrics for fair model comparison, and detailed performance analysis. Extensive evaluations of published and baseline models reveal limitations like mode or posterior collapse, and underscore the importance of rank metrics that assess the ordering of perturbations alongside traditional measures like RMSE. Our findings show that simple models can outperform more complex approaches. This benchmarking exercise sets new standards for model evaluation, supports robust model development, and advances the potential of these models to use high-throughput and high-content genetic and chemical screens for disease target discovery.","['perturb-seq', 'single cell', 'perturbation prediction', 'benchmarking', 'cellular perturbation', 'disentanglement']",,https://openreview.net/pdf?id=1EJXtK3AWr,spotlight
https://openreview.net/forum?id=z54ZEqbU2m,PQA: Zero-shot Protein Question Answering for Free-form Scientific Enquiry with Large Language Models,"['Eli M Carrami', 'Sahand Sharifzadeh']","Understanding protein structure and function is crucial in biology. However, current computational methods are often task-specific and resource-intensive. To address this, we propose zero-shot Protein Question Answering (PQA), a task designed to answer a wide range of protein-related queries without task-specific training. The success of PQA hinges on high-quality datasets and robust evaluation strategies, both of which are lacking in current research. Existing datasets suffer from biases, noise, and lack of evolutionary context, while current evaluation methods fail to accurately assess model performance. We introduce the Pika framework to overcome these limitations. Pika comprises a curated, debiased dataset tailored for PQA and a biochemically relevant benchmarking strategy. We also propose multimodal large language models as a strong baseline for PQA, leveraging their natural language processing and knowledge. This approach promises a more flexible and efficient way to explore protein properties, advancing protein research. Our comprehensive PQA framework, Pika, including dataset, code, and model checkpoints, is openly accessible on Github and Hugging Face, promoting wider research in the field.","['Protein Function', 'Question Answering', 'Multimodal Language Models']",,https://openreview.net/pdf?id=z54ZEqbU2m,spotlight
https://openreview.net/forum?id=kL8dlYp6IM,Signals in the Cells: Multimodal and Contextualized Machine Learning Foundations for Therapeutics,"['Alejandro Velez-Arce', 'Kexin Huang', 'Michelle M Li', 'xiang lin', 'Wenhao Gao', 'Bradley Pentelute', 'Tianfan Fu', 'Manolis Kellis', 'Marinka Zitnik']","Drug discovery AI datasets and benchmarks have not traditionally included single-cell analysis biomarkers. While benchmarking efforts in single-cell analysis have recently released collections of single-cell tasks, they have yet to comprehensively release datasets, models, and benchmarks that integrate a broad range of therapeutic discovery tasks with cell-type-specific biomarkers. Therapeutics Commons (TDC-2) presents datasets, tools, models, and benchmarks integrating cell-type-specific contextual features with ML tasks across therapeutics. We present four tasks for contextual learning at single-cell resolution: drug-target nomination, genetic perturbation response prediction, chemical perturbation response prediction, and protein-peptide interaction prediction. We introduce datasets, models, and benchmarks for these four tasks. Finally, we detail the advancements and challenges in machine learning and biology that drove the implementation of TDC-2 and how they are reflected in its architecture, datasets and benchmarks, and foundation model tooling.","['machine learning in therapeutics', 'datasets and benchmarks for ML therapeutics', 'biomedical artificial intelligence']",,https://openreview.net/pdf?id=kL8dlYp6IM,spotlight
https://openreview.net/forum?id=OoCtLuIbD7,Orthrus: Towards Evolutionary and Functional RNA Foundation Models,"['Philip Fradkin', 'Ruian Shi', 'Keren Isaev', 'Brendan Frey', 'Quaid Morris', 'Leo J Lee', 'BO WANG']","In the face of rapidly accumulating genomic data, our understanding of the RNA regulatory code remains incomplete. Pre-trained genomic foundation models offer an avenue to adapt learned RNA representations to biological prediction tasks. However, existing genomic foundation models are trained using strategies borrowed from textual or visual domains, such as masked language modelling or next token prediction, that do not leverage biological domain knowledge. Here, we introduce Orthrus, a mamba based RNA foundation model pre-trained using a novel self-supervised contrastive learning objective with biological augmentations. Orthrus is trained by maximizing embedding similarity between curated pairs of RNA transcripts, where pairs are formed from splice isoforms of 10 model organisms and transcripts from orthologous genes in 400+ mammalian species from the Zoonomia Project. This training objective results in a latent representation that clusters RNA sequences with functional and evolutionary similarities. We find that the generalized mature RNA isoform representations learned by Orthrus significantly outperform existing genomic foundation models on five mRNA property prediction tasks, and requires only a fraction of fine-tuning data to do so.","['Contrastive Learning', 'Genomics', 'Self-Supervised Learning', 'RNA', 'Orthology', 'Representation Learning', 'Alternative Splicing', 'Few-shot Learning']",,https://openreview.net/pdf?id=OoCtLuIbD7,spotlight
https://openreview.net/forum?id=zC8AJaKAbb,A Deep Learning Approach for RNA-Compound Interaction Prediction with Binding Site Interpretability,"['Haelee Bae', 'Hojung Nam']","RNA-compound interaction prediction is crucial for expanding the therapeutic target space beyond proteins. However, existing models are limited by data scarcity and often lack interpretability. We present DeepRNA-DTI, the first sequence-based deep learning model for RNA-compound interaction prediction. Our model leverages pretrained embeddings from RNA-FM for RNA sequences and MoleBERT for compounds, capturing complex interaction patterns through attention mechanisms. DeepRNA-DTI jointly predicts drug-target interactions (DTI) and RNA binding sites, enhancing interpretability. Trained on datasets from the Protein Data Bank (PDB) and literature, DeepRNA-DTI demonstrates improved performance in RNA-compound interaction tasks compared to existing methods. Our approach offers valuable insights into binding sites and opens new avenues for RNA-targeted drug discovery.","['RNA-compound interaction', 'deep learning', 'binding information', 'RNA sequence']",,https://openreview.net/pdf?id=zC8AJaKAbb,spotlight
https://openreview.net/forum?id=DgM6coUWZp,DeepProtein: Deep Learning Library and Benchmark for Protein Sequence Learning,"['Jiaqing Xie', 'Yue Zhao', 'Tianfan Fu']","In recent years, deep learning has revolutionized the field of protein science, enabling advancements in predicting protein properties, structural folding and interactions. This paper presents DeepProtein, a comprehensive and user-friendly deep learning library specifically designed for protein-related tasks. DeepProtein integrates a couple of state-of-the-art neural network architectures, which include convolutional neural network (CNN), recurrent neural network (RNN), transformer, graph neural network (GNN), and graph transformer (GT). It provides user-friendly interfaces, facilitating domain researchers in applying deep learning techniques to protein data. Also, we curate a benchmark that evaluates these neural architectures on a variety of protein tasks, including protein function prediction, protein localization prediction, and protein-protein interaction prediction, showcasing its superior performance and scalability. Additionally, we provide detailed documentation and tutorials to promote accessibility and encourage reproducible research. This is a library that is extended from a well-
known drug discovery library, DeepPurpose. The library is publicly available at
https://anonymous.4open.science/r/DeepProtein-F8FE.","['large molecule learning', 'protein sequence learning', 'antibody', 'benchmark', 'library', 'graph neural network', 'transformer', 'convolutional neural network']",,https://openreview.net/pdf?id=DgM6coUWZp,spotlight
https://openreview.net/forum?id=0KAFbnTAdW,Computational Antigen Optimization through Symbolic Optimization and Affinity Maturation Simulation,"['Jonathan G. Faris', 'Mikel Landajuela', 'Kayla G. Sprenger', 'Daniel faissol', 'Felipe Leno da Silva']","With the recent, significant improvement of computational tools for protein interaction prediction, the use of machine learning to support the development of vaccination regimens brings with it new hope for diseases which, so far, have eluded our best efforts at finding a cure, like HIV. We here propose BIOVAX, a novel pipeline combining symbolic optimization with affinity maturation simulation to generate highly-optimized antigens intended for vaccination development. We perform an in silico evaluation using real HIV targets, and show that the antigen designed by BIOVAX elicit estimated antibodies that bind more strongly to a diverse, global panel of real HIV viruses than both the parent sequence, and other computationally-designed antigen baselines available in the literature. BIOVAX is our first step towards a new generation of AI-assisted vaccine development pipelines.","['symbolic optimization', 'antigen design', 'vaccine design', 'HIV', 'affinity maturation']",,https://openreview.net/pdf?id=0KAFbnTAdW,spotlight
https://openreview.net/forum?id=df28S7MoDt,"Alignment-based and protein foundation models for viral evolution, vaccines and vectors","['Sarah Gurev', 'Noor Youssef', 'Navami Jain', 'Debora Susan Marks']","Protein mutation effect prediction has advanced several fields, from designing enzymes to forecasting viral evolution. These models are typically trained on sequence data, structural data, or a combination of both. Sequence-based models learn constraints governing protein structure and function from sequence data and fall broadly into two categories: alignment-based models and protein language models (PLMs). We provide the first detailed comparison of modeling approaches specifically for viruses. We curated a dataset of over 59 standardized viral deep mutational scanning assays and assessed the relative performance of three alignment-based models (PSSM, EVmutation, and EVE), three PLMs (ESM-1v, Tranception, and VESPA), and two versions of SaProt, a structurally-aware protein language model (SaProt-AF2 and SaProt-PDB). Interestingly, deeper alignments often led to worse performance for alignment-based models. On the other hand, PLMs tended to perform better as the size of the training database increased. Overall, alignment-based models outperformed sequence-only PLMs, while the best alignment based model performed on par with SaProt-PDB. Our findings suggest that modeling strategies that are effective for other taxa may not translate directly to viruses, likely due to the limited number of viral sequences used for training. However, incorporating additional virus-specific data into PLMs could enhance their predictive power for viral mutation effects, important for understanding viral evolution and the design of vaccines and viral vectors.","['protein language models', 'viral evolution', 'vaccines']",,https://openreview.net/pdf?id=df28S7MoDt,spotlight
https://openreview.net/forum?id=p3uRSmuJmX,Directly Optimizing for Synthesizability in Generative Molecular Design using Retrosynthesis Models,"['Jeff Guo', 'Philippe Schwaller']","Synthesizability in generative molecular design remains a pressing challenge. Existing methods to assess synthesizability span heuristics-based methods, retrosynthesis models, and synthesizability-constrained molecular generation. The latter has become increasingly prevalent and proceeds by defining a set of permitted actions a model can take when generating molecules, such that all generations are anchored in ""synthetically-feasible"" chemical transformations. To date, retrosynthesis models have been mostly used as a post-hoc filtering tool as their inference cost remains prohibitive to use directly in an optimization loop. In this work, we show that with a sufficiently sample-efficient generative model, it is straightforward to directly optimize for synthesizability using retrosynthesis models in goal-directed generation. Under a heavily-constrained computational budget, our model can generate molecules satisfying a multi-parameter drug discovery optimization task while being synthesizable, as deemed by the retrosynthesis model.","['generative molecular design', 'synthesizability', 'general-purpose models', 'drug discovery']",,https://openreview.net/pdf?id=p3uRSmuJmX,spotlight
https://openreview.net/forum?id=yvco9cpMjp,IgBlend: Unifying 3D Structure and Sequence for Antibody LLMs,"['Cedric Malherbe', 'Talip Ucar']","Large language models (LLMs) trained on antibody sequences have shown significant potential in the rapidly advancing field of machine learning-assisted antibody engineering and drug discovery. However, current state-of-the-art antibody LLMs often overlook structural information, which could enable the model to more effectively learn the functional properties of antibodies by providing richer, more informative data. In response to this limitation, we introduce IgBend, which integrates both the 3D coordinates of backbone atoms (C-alpha, N, and C) and antibody sequences. Our model is trained on a diverse dataset containing over 4 million unique structures and more than 200 million unique sequences, including heavy and light chains as well as nanobodies. We rigorously evaluate IgBend using established benchmarks such as sequence recovery, complementarity-determining region (CDR) editing and inverse folding and demonstrate that \modelname~consistently outperforms current state-of-the-art models across all benchmarks.","['LLMs', 'antibodies', 'deep-learning', 'structure']",,https://openreview.net/pdf?id=yvco9cpMjp,spotlight
https://openreview.net/forum?id=yaGLzGiVZS,Fine-Tuning Discrete Diffusion Models via Reward Optimization with Applications to DNA and Protein Design,"['Chenyu Wang', 'Masatoshi Uehara', 'Yichun He', 'Amy Wang', 'Tommaso Biancalani', 'Avantika Lal', 'Tommi Jaakkola', 'Sergey Levine', 'Hanchen', 'Aviv Regev']","Recent studies have demonstrated the strong empirical performance of diffusion models on discrete sequences (i.e., discrete diffusion models) across domains from natural language to biological sequence generation. For example, in the protein inverse folding task, where the goal is to generate a protein sequence from a given backbone structure, conditional diffusion models have achieved impressive results in generating natural-like sequences that fold back into the original structure. However, practical design tasks often require not only modeling a conditional distribution but also optimizing specific task objectives. For instance, in the inverse folding task, we may prefer protein sequences with high stability. To address this, we consider the scenario where we have pre-trained discrete diffusion models that can generate natural-like sequences, as well as reward models that map sequences to task objectives. We then formulate the reward maximization problem within discrete diffusion models, analogous to reinforcement learning (RL), while minimizing the KL divergence against pretrained diffusion models to preserve naturalness. To solve this RL problem, we propose a novel algorithm, DRAKES, that enables direct backpropagation of rewards through entire trajectories generated by diffusion models,
by making the originally non-differentiable trajectories differentiable using the Gumbel-Softmax trick.
Our theoretical analysis indicates that our approach can generate sequences that are both natural-like (i.e., have a high probability under a pretrained model) and yield high rewards.
While similar tasks have been recently explored in diffusion models for continuous domains, our work addresses unique algorithmic and theoretical challenges specific to discrete diffusion models, which arise from their foundation in continuous-time Markov chains rather than Brownian motion. Finally, we demonstrate the effectiveness of our algorithm in generating DNA and protein sequences that optimize enhancer activity and protein stability, respectively, important tasks for gene therapies and protein-based therapeutics.","['Discrete Diffusion Models', 'Reward Optimization', 'Fine-Tuning', 'AI for science', 'Reinforcement learning']",,https://openreview.net/pdf?id=yaGLzGiVZS,accept
https://openreview.net/forum?id=FKCYEbSaE1,Disentangling the Peptide Space: A Contrastive Approach with Wasserstein Autoencoders,"['Mihir Agarwal', 'Progyan Das']","Antimicrobial peptides (AMPs) have been shown to be promising therapeutic approaches against antibiotic-resistant pathogens. In the ongoing search for new AMPs, data-driven methods, especially generative models, have become indispensable tools for expediting discovery. We introduce a novel architecture, \textbf{Contrastive Wasserstein Autoencoder (C-WAE)}, designed for the \textit{de novo} generation of AMP candidates by establishing a discriminative latent space of amino acid sequences. The architecture combines Wasserstein distance metrics with a contrastive loss function to achieve a highly separable latent space where AMPs and non-AMPs are distinctly classified. Further, a predictive models trained on a separate validation set could correctly classify as antimicrobial >90\% of samples. Empirical evaluations confirm that the C-WAE succeeds in generating high-quality candidate AMPs as predicted by classifier. Our contributions are twofold: 1) A new architecture for candidate AMP generation using contrastive learning, and 2) To the best of our understanding, this is the first study that integrates contrastive learning for the \textit{de novo} synthesis of AMPs.","['Autoencoders', 'Peptides']",,https://openreview.net/pdf?id=FKCYEbSaE1,accept
https://openreview.net/forum?id=jAXfu6PqLp,Training-Free Guidance with Applications to Protein Engineering,"['Lewis Cornwall', 'Joshua Meyers', 'James Day', 'Lilly S Wollman', 'Neil Dalchau', 'Aaron Sim']","Diffusion models facilitate powerful control over the generative process. Here we introduce training-free guidance, a method for sampling from a broad class of conditional distributions that can be considered generalisations of inpainting. The method is grounded in annealed Langevin dynamics which ensures convergence to the exact conditional distribution, unlike popular methods for inpainting which rely on heuristics. We demonstrate training-free guidance using pretrained unconditional models for protein structure, and protein sequence generation and improve upon state-of-the-art approaches. We show the versatility of training-free guidance by addressing a wider range of tasks, including multi-motif scaffolding and amino acid mutagenesis of T cell receptors, with applications to biologics design.","['inpainting', 'diffusion', 'proteins', 'langevin', 'sequence editing', 'floating inpainting']",,https://openreview.net/pdf?id=jAXfu6PqLp,accept
https://openreview.net/forum?id=ZBrSA6x9HN,Learning Protocols for Non-Equilibrium Conformational Free-Energy Estimation Using Optimal Transport and Conditional Flow Matching,"['Lars Holdijk', 'Michael M. Bronstein', 'Max Welling']","Accurate estimation of conformational free-energy differences is crucial in drug discovery, particularly for understanding protein-ligand stability and transitions that dictate drug binding and selectivity. Despite its importance, machine learning (ML) applications in this area remain limited, with existing methods relying on exhaustive equilibrium sampling of the Boltzmann distribution. We present an alternative approach using the Generalised Jarzynski Equality from non-equilibrium thermodynamics to estimate free-energy differences without equilibrium sampling. Specifically, we focus on deriving optimal minimal work protocols to switch between conformational states, leveraging Optimal Transport theory and proposing an efficient Conditional Flow Matching-based method to learn these protocols. Our experiments demonstrate the effectiveness of this approach in accurately estimating conformational free energies. Future work will aim to scale this method to larger, more complex systems relevant to drug development.",['Conformational Free-Energy; Jarzynski; Optimal Protocols; Optimal Transport; Conditional Flow Matching'],,https://openreview.net/pdf?id=ZBrSA6x9HN,accept
https://openreview.net/forum?id=WLAMEZzu0p,GeneGench: Systematic Evaluation of Genomic Foundation Models and Beyond,"['Zicheng Liu', 'Jiahui Li', 'Lei Xin', 'Siyuan Li', 'Chang Yu', 'Zelin Zang', 'Cheng Tan', 'Yufei Huang', 'yajingbai', 'Jun Xia', 'Stan Z. Li']","The Genomic Foundation Model (GFM) paradigm is expected to facilitate the extraction of generalizable representations from massive genomic data, thereby enabling their application across a spectrum of downstream applications. Despite advancements, a lack of evaluation framework makes it difficult to ensure equitable assessment due to experimental settings, model intricacy, benchmark datasets, and reproducibility challenges. In the absence of standardization, comparative analyses risk becoming biased and unreliable. To surmount this impasse, we introduce GeneBench, a comprehensive benchmarking suite specifically tailored for evaluating the efficacy of Genomic Foundation Models. GeneBench offers a modular and expandable framework that encapsulates a variety of state-of-the-art methodologies. Through systematic evaluations of datasets spanning diverse biological domains with a particular emphasis on both short-range and long-range genomic tasks, firstly including the three most important DNA tasks covering Coding Region, Non-Coding Region, Genome Structure, etc. Our results on GenBench have led to an interesting discovery: regardless of the number of parameters, the noticeable variation in preference between attention-based and convolution-based models for short- and long-range tasks could offer valuable insights for the future development of GFM. As a result, we propose a straightforward modified model called Genhybrid, which is an effective and efficient convolution-attention hybrid model suitable for all tasks.","['genetic foundation model', 'benchmark', 'hybrid model']",,https://openreview.net/pdf?id=WLAMEZzu0p,accept
https://openreview.net/forum?id=1ib5oyTQIb,Molecular Generation with State Space Sequence Models,"['Anri Lombard', 'Shane Acton', 'Ulrich Armel Mbou Sob', 'Jan Buys']","Molecular generation is a critical task in drug discovery but current approaches often struggle with efficiency and scalability when dealing with complex molecular structures. 
This paper aims to address these challenges by training and evaluating models for molecular generation using the MAMBA State Space Model architecture.
We develop models with 20M and 90M parameters trained on the MOSES and ZINC datasets, respectively, using the Sequential Attachment-based Fragment Embedding (SAFE) representation. 
We compare MAMBA models against the prevailing Transformer architecture in terms of generation quality and computational efficiency.
Our findings suggest that MAMBA models can achieve performance comparable to Transformers in generating valid, unique, and diverse molecules. Generation from both architectures can achieve close to perfect validity and uniqueness scores, although MAMBA models require more conservative sampling parameters or regeneration steps to achieve these results. MAMBA models consistently demonstrates lower perplexity and reduced GPU power consumption (up to 30\% reduction) compared to Transformer models. 
These results indicate that State Space Models may offer a computationally efficient alternative for molecular generation tasks, potentially enabling more efficient processing of larger datasets and complex molecular structures. 
The efficiency gains of MAMBA models become more pronounced with longer sequences, suggesting that this architecture could enable the modeling and generation of more complex molecules. This capability could significantly expand the scope of AI-driven molecular design in drug discovery.","['state space architecture', 'transformer architecture', 'drug discovery', 'de novo generation']",,https://openreview.net/pdf?id=1ib5oyTQIb,accept
https://openreview.net/forum?id=vSVjNIFkwu,MOFFlow: Flow Matching for Structure Prediction of Metal-Organic Frameworks,"['Nayoung Kim', 'Seongsu Kim', 'Minsu Kim', 'Jinkyoo Park', 'Sungsoo Ahn']","Metal-organic frameworks (MOFs) are a class of crystalline materials with promising applications in many areas such as carbon capture and drug delivery. In this work, we introduce MOFFlow, the first deep generative model tailored for MOF structure prediction. Existing approaches, including ab initio calculations and even deep generative models, struggle with the complexity of MOF structures due to the large number of atoms in the unit cells. To address this limitation, we propose a novel Riemannian flow matching framework that reduces the dimensionality of the problem by treating the metal nodes and organic linkers as rigid bodies, capitalizing on the inherent modularity of MOFs. By operating in the $SE(3)$ space, MOFFlow effectively captures the roto-translational dynamics of these rigid components in a scalable way. Our experiment demonstrates that MOFFlow accurately predicts MOF structures containing several hundred atoms, significantly outperforming conventional methods and state-of-the-art machine learning baselines while being much faster.","['metal-organic framework', 'material structure prediction', 'AI for science']",,https://openreview.net/pdf?id=vSVjNIFkwu,accept
https://openreview.net/forum?id=G7oGoygGZk,Homomorphism Counts as Structural Encodings for Molecular Property Prediction,"['Linus Bao', 'Emily Jin', 'Michael M. Bronstein', 'Ismail Ilkan Ceylan', 'Matthias Lanzinger']","Graph transformers are popular neural networks that extend the well-known transformer architecture to the graph domain. These architectures operate by applying self-attention on graph nodes and incorporating graph structure through the use of positional encodings (e.g., Laplacian positional encoding) or structural encodings (e.g., random-walk structural encoding). The quality of such encodings is critical, since they provide the necessary \emph{graph inductive biases} to condition the model on graph structure. In this work, we propose \emph{motif structural encoding} (\emph{MoSE}) as a flexible and powerful structural encoding framework based on counting graph homomorphisms. Theoretically, we compare the expressive power of MoSE to random-walk structural encoding and relate both encodings to the expressive power of standard message passing neural networks. Empirically, we observe that MoSE outperforms other well-known positional and structural encodings across a range of architectures, and it achieves state-of-the-art performance on widely studied molecular property prediction datasets.","['graph transformers', 'structural encodings', 'homomorphism counts', 'expressivity']",,https://openreview.net/pdf?id=G7oGoygGZk,accept
https://openreview.net/forum?id=r9OUQpwhVo,Probing the Embedding Space of Protein Foundation Models through Intrinsic Dimension Analysis,"['Soojung Yang', 'Juno Nam', 'Tynan Perez', 'Jinyeop Song', 'Xiaochen Du', 'Rafael Gomez-Bombarelli']","Protein foundation models produce embeddings that are valuable for various downstream tasks, yet the structure and information content of these embeddings remain poorly understood, particularly in relation to diverse pre-training tasks and input modalities. 
We apply intrinsic dimension ($I_d$) analysis to quantify the complexity of protein embeddings from several widely used models, including ESM-2, ESM-IF, ProstT5, and ProteinMPNN. We also employ $I_d$ correlation ($I_d$Cor) to measure the shared information between different embeddings. Our results reveal a universality in protein embeddings, with similar $I_d$ scales across models and strong correlations between protein and residue embeddings. We observe significant redundancy, with $I_d$ values much smaller than the original embedding dimensions. We also show that models capture both spatial and sequential long-range correlation, with correlation decay rate differing based on the input modalities and pre-training tasks.  Lastly, we analyze mutant embeddings, revealing that mutations cluster effectively by site, and fine-tuning further reduces the $I_d$ to capture task-specific representations.","['Foundation model', 'intrinsic dimension', 'ESM', 'MPNN', 'finetuning', 'pretraining']",,https://openreview.net/pdf?id=r9OUQpwhVo,accept
https://openreview.net/forum?id=PUJRlIXaHk,MV-CLAM: Multi-View Molecular Interpretation with Cross-Modal Projection via Language Model,"['Sumin Ha', 'Jun Hyeong Kim', 'Yinhua Piao', 'Sun Kim']","Large language models (LLMs) have shown significant potential in the biomolecular domain, particularly by demonstrating that effective adaptation of molecular representations for LLMs can greatly improve the quality of molecular captions.
Most previous works have focused on aligning unimodal molecular structures with text, overlooking the diversity of modalities. 
Naive approaches to aligning multi-modal molecular structures with text often lead to (1) separately aligned embeddings, (2) inconsistent textual representations, and (3) increased computational overhead. 
To address these challenges, we propose LLM framework MV-CLAM equipped with MQ-Former, a novel multi-querying transformer. 
This architecture introduces a cross-model projector facilitating the simultaneous alignment of 2D and 3D molecular representations to a unified text token. 
By employing a shared self-attention layer, MQ-Former preserves rich molecular embeddings across different dimensions while consolidating them into a universal molecular token. 
Our approach outperforms baseline models in both molecule-text retrieval and molecule captioning tasks. 
Additionally, our framework shows promising results for zero-shot molecule editing, showcasing its capacity to extend beyond description generation.
By effectively integrating multi-view molecular data into a format conducive to LLMs, our method serves as a valuable tool for enhancing the characterization and understanding of chemical structures, facilitating a more seamless transition from molecular data to textual descriptions. The source code of MV-CLAM is available in https://github.com/sumin124/mv-clam.git.","['Molecule captioning', 'large language models', 'drug discovery', 'molecule representation learning']",,https://openreview.net/pdf?id=PUJRlIXaHk,accept
https://openreview.net/forum?id=nXmisIjYfu,Reinforcement Learning for Enhanced Targeted Molecule Generation Via Language Models,"['Salma J. Ahmed', 'Emad A. Mohammed']","Developing new drugs is laborious and costly, demanding extensive time investment. In this study, we introduce an innovative de-novo drug design strategy, which harnesses the capabilities of language models to devise targeted drugs for specific proteins. Employing a Reinforcement Learning (RL) framework utilizing Proximal Policy Optimization (PPO), we refine the model to acquire a policy for generating drugs tailored to protein targets. Our method integrates a composite reward function, combining considerations of drug-target interaction and molecular validity. Following RL fine-tuning, our approach demonstrates promising outcomes, yielding notable improvements in molecular validity, interaction efficacy, and critical chemical properties, achieving 65.37 for Quantitative Estimation of Drug-likeness (QED), 321.55 for Molecular Weight (MW), and 4.47 for Octanol-Water Partition Coefficient (logP), respectively. Furthermore, out of the generated drugs, only 0.041% do not exhibit novelty.","['Drug Discovery', 'Reinforcement Learning', 'Language Models']",,https://openreview.net/pdf?id=nXmisIjYfu,accept
https://openreview.net/forum?id=NjlafBAahz,Interpretable Causal Representation Learning for Biological Data in the Pathway Space,"['Jesus de la Fuente CedeÃ±o', 'Robert Lehmann', 'Carlos Ruiz-Arenas', 'Irene MarÃ­n-GoÃ±i', 'Jan Voges', 'Xabier Martinez de Morentin', 'David Gomez-Cabrero', 'Idoia Ochoa', 'Jesper TegnÃ©r', 'Vincenzo Lagani', 'Mikel Hernaez']","Predicting the impact of genomic and drug perturbations in cellular function is crucial for understanding gene functions and drug effects, ultimately leading to improved therapies. To this end, Causal Representation Learning (CRL) constitutes one of the most promising approaches, as it aims to identify the latent factors that causally govern biological systems, thus facilitating the prediction of the effect of unseen perturbations. Yet, current CRL methods fail in reconciling their principled latent representations with known biological processes, leading to models that are not interpretable. To address this major issue, in this work we present SENA-discrepancy-VAE, a model based on the recently proposed CRL method discrepancy-VAE, that produces representations where each latent factor can be interpreted as the (linear) combination of the activity of a (learned) set of biological processes. To this extent, we present an encoder, SENA-, that efficiently compute and map biological processes' activity levels to the latent causal factors. We show that SENA-discrepancy-VAE achieves predictive performances on unseen combinations of interventions that are comparable with its original, non-interpretable counterpart, while inferring causal latent factors that are biologically meaningful.","['Causal Representation Learning', 'Intepretability', 'VAE', 'Genomic Perturbations', 'Perturb-Seq', 'Single Cell']",,https://openreview.net/pdf?id=NjlafBAahz,accept
https://openreview.net/forum?id=3rtxtV5QDJ,PLMFit: Benchmarking Transfer Learning with Protein Language Models for Protein Engineering,"['Thomas Bikias', 'Evangelos Stamkopoulos', 'Sai T. Reddy']","Protein language models (PLMs) have emerged as a useful resource for protein engineering applications. Transfer learning (TL) leverages pre-trained parameters to extract features to train machine learning models or adjust the weights of PLMs for novel tasks via fine-tuning through back-propagation. TL methods have shown potential for enhancing protein predictions performance when paired with PLMs, however there is a notable lack of comparative analyses that benchmark TL methods applied to state-of-the-art PLMs, identify optimal strategies for transferring knowledge and determine the most suitable approach for specific tasks. Here, we report PLMFit, a benchmarking study that combines, three state-of-the-art PLMs (ESM2, ProGen2, ProteinBert), with three TL methods (feature extraction, low-rank adaptation, bottleneck adapters) for five protein engineering datasets. We conducted over 2900 experiments, altering PLM sizes and layers, TL hyperparameters and different training procedures. Our experiments reveal three key findings: (i) utilizing a fraction of PLM for transfer learning does not detrimentally impact performance, (ii) the choice between feature extraction and fine-tuning is primarily dictated by the amount and diversity of data and (iii) fine-tuning is most effective when generalization is necessary and only limited data is available. We provide PLMFit as an open-source software package, serving as a valuable resource for the scientific community to facilitate the feature extraction and fine-tuning of PLMs for various applications.","['Parameter efficient fine-tuning', 'Low rank adaptation', 'Protein language models', 'Feature extraction', 'Benchmarking', 'Protein engineering']",,https://openreview.net/pdf?id=3rtxtV5QDJ,accept
https://openreview.net/forum?id=WZ0xQcEvwU,Improved Off-policy Reinforcement Learning in  Biological Sequence Design,"['Hyeonah Kim', 'Minsu Kim', 'Taeyoung Yun', 'Sanghyeok Choi', 'Emmanuel Bengio', 'Alex HernÃ¡ndez-GarcÃ­a', 'Jinkyoo Park']","Designing biological sequences with desired properties is a significant challenge due to the combinatorially vast search space and the high cost of evaluating each candidate sequence. To address these challenges, reinforcement learning (RL) methods, such as GFlowNets, utilize proxy models for rapid reward evaluation and annotated data for policy training. Although these approaches have shown promise in generating diverse and novel sequences, the limited training data relative to the vast search space often leads to the misspecification of proxy for out-of-distribution inputs. We introduce $\delta$-Conservative Search, a novel off-policy search method for training GFlowNets designed to improve robustness against proxy misspecification. The key idea is to incorporate conservativeness, controlled by parameter $\delta$, to constrain the search to reliable regions. Specifically, we inject noise into high-score offline sequences by randomly masking tokens with a Bernoulli distribution of parameter $\delta$ and then denoise masked tokens using the GFlowNet policy. Additionally, $\delta$ is adaptively adjusted based on the uncertainty of the proxy model for each data point. This enables the reflection of proxy uncertainty to determine the level of conservativeness. Experimental results demonstrate that our method consistently outperforms existing machine learning methods in discovering high-score sequences across diverse tasksâ€”including DNA, RNA, protein, and peptide designâ€”especially in large-scale scenarios.","['Biological sequence design', 'GFlowNets', 'offline RL', 'active learning']",,https://openreview.net/pdf?id=WZ0xQcEvwU,accept
https://openreview.net/forum?id=NKHNAtDiCd,Distilling Structural Representations into Protein Sequence Models,"['Jeffrey Ouyang-Zhang', 'Chengyue Gong', 'Yue Zhao', 'Philipp Kraehenbuehl', 'Adam Klivans', 'Daniel Jesus Diaz']","Protein language models, like the popular ESM2, are widely used tools for extracting evolution-based protein representations and have achieved significant success on downstream biological tasks.
Representations based on sequence and structure models, however, show significant performance differences depending on the downstream task.
A major open problem is to obtain representations that best capture both the evolutionary and structural properties of proteins in general. 
Here we introduce Implicit Structure Model (ISM), a sequence-only input model with structurally-enriched representations that outperforms state-of-the-art sequence models on several well-studied benchmarks including mutation stability assessment and structure prediction. 
Our key innovations are a microenvironment-based autoencoder for generating structure tokens and a self-supervised training objective that distills these tokens into ESM2's pre-trained model. 
We have made ISM's structure-enriched weights easily available: integrating ISM into any application using ESM2 requires changing only a single line of code.
Our code is available at \url{https://github.com/jozhang97/ISM}.","['biology', 'proteins', 'sequence', 'structure', 'autoencoder', 'esm']",,https://openreview.net/pdf?id=NKHNAtDiCd,accept
https://openreview.net/forum?id=ofcL9aXxp6,CancerFoundation: A single-cell RNA sequencing foundation model to decipher drug resistance in cancer,"['Alexander Theus', 'Florian Barkmann', 'David Wissel', 'Valentina Boeva']","We present CancerFoundation, a novel single-cell RNA-seq foundation model (scFM) trained exclusively on malignant cells. Despite being trained on only one million total cells, a fraction of the data used by existing models, CancerFoundation outperforms other scFMs in key tasks such as zero-shot batch integration and drug response prediction. During training, we employ tissue and technology-aware oversampling and domain-invariant training to enhance performance on underrepresented cancer types and sequencing technologies. We propose survival prediction as a new downstream task to evaluate the generalizability of single-cell foundation models to bulk RNA data and their applicability to patient stratification. CancerFoundation demonstrates superior batch integration performance and shows significant improvements in predicting drug responses for both unseen cell lines and drugs. These results highlight the potential of focused, smaller foundation models in advancing drug discovery and our understanding of cancer biology.","['Drug discovery', 'Single-cell foundation model', 'Single-cell RNA sequencing', 'Cancer biology']",,https://openreview.net/pdf?id=ofcL9aXxp6,accept
https://openreview.net/forum?id=kUVxRERDGL,TCRGenesis: Generation of SIINFEKL-specific T-cell receptor sequences using autoregressive Transformer,"['Yang An', 'Felix Drost', 'Adrian Straub', 'Annalisa Marsico', 'Dirk H Busch', 'Benjamin Schubert']","Engineered T-cell therapies are a promising new approach for treating previously uncurable diseases. These therapies involve genetically modified T cells expressing custom T cell receptors (TCRs) that recognize antigens from cancer, virus-infected, or autoimmune cells. However, the identification or generation of suitable TCRs remains an unsolved challenge. Computational methods hold the potential to accelerate the development of TCRs binding towards target antigens. While the computational investigation of the TCR-epitope landscape has been mainly focused on binding prediction, synthetic TCR design has recently emerged as the next frontier. Here, we present a proof-of-concept study on generating full TCR sequences reactive to a fixed epitope $\textit{in silico}$. Towards this, we utilized a unique dataset comprising thousands of TCRs experimentally validated as reactive towards the model epitope-MHC complex SIINFEKL/H2-K$^b$ and a naive TCR background to train our autoregressive transformer model TCRGenesis. The model generated a repertoire of realistic TCRs as validated through various biophysical and sequence properties. Further, the sequences exhibited high binding scores according to a predictor specifically developed for evaluation. The generator inherently captured the rules governing binding towards SIINFEKL as its perplexity score assigned to real, unseen TCR sequences separates well between binding and non-binding TCRs, and the generated sequences resembled binders. This work marks one of the first steps in the full-sequence design of TCRs specific to an antigen $\textit{in silico}$, which we envision will accelerate the development of future immunotherapies and personalized medicine through rapid and reliable TCR synthesis.","['biological sequence design', 'protein design', 'protein generation']",,https://openreview.net/pdf?id=kUVxRERDGL,accept
https://openreview.net/forum?id=q3uKWlUzHb,Modeling CAR Response at the Single-Cell Level Using Conditional OT,"['Alice Driessen', 'Jannis Born', 'RocÃ­o Castellanos Rueda', 'Sai T. Reddy', 'Marianna Rapsomaniki']","Chimeric Antigen Receptor (CAR) T cell therapy is a promosing area of cancer immunotherapy. However, many challenges such as loss of persistence, T cell exhaustion, and therapy associated toxicities hamper further advancement of CAR T cell therapy. Therefore, recent efforts have focused on designing improved CARs that show better therapeutic characteristics. However, it is unfeasible to test all CAR variants in lab-based assays as CARs consist of multiple intracellular signalling domains. This results in over 100â€™000 possible variants. We leverage computational modeling to navigate this vast combinatorial space by learning the relationship between CAR design and T cell functionality, thereby proposing promising CAR T cell designs. CAR T cells expressing different variants can be viewed as cells that underwent different perturbations. Neural Optimal Transport is an upcoming field that can model single cell perturbations and predict unseen cells and conditions. In this work we leverage the conditional Monge Gap to model the response to CAR expression at a single-cell level and generate gene expression of cells that express an unseen CAR design. We show that CAR OT (CAROT) significantly outperforms the baseline for gene expression prediction for in-distribution CAR variants, with distinct gene expression patterns per CAR that capture biological characteristics. When predicting unseen CAR variants, we demonstrate promising results in terms of gene expression prediction and show the model learns gene expression patterns linked to domains in the training set. This work demonstrates that optimal transport may support discovery and development of new CAR T cell designs.","['Optimal Transport', 'OT', 'CAR T cell', 'Chimeric Antigen Receptor', 'Cell Therapy', 'Cell Therapies', 'Monge Gap', 'Immunotherapy', 'Cancer', 'Conditional Monge Gap', 'Conditional Optimal Transport']",,https://openreview.net/pdf?id=q3uKWlUzHb,accept
https://openreview.net/forum?id=p0KYWGpAHB,Discrete Diffusion SchrÃ¶dinger Bridge Matching for Graph Transformation,"['Jun Hyeong Kim', 'Seonghwan Kim', 'Seokhyun Moon', 'Hyeongwoo Kim', 'Jeheon Woo', 'Woo Youn Kim']","Transporting between arbitrary distributions is a fundamental goal in generative modeling.
Recently proposed diffusion bridge models provide a potential solution, but they rely on a joint distribution that is difficult to obtain in practice.
Furthermore, formulations based on continuous domains limit their applicability to discrete domains such as graphs.
To overcome these limitations, we propose Discrete Diffusion SchrÃ¶dinger Bridge Matching (DDSBM), a novel framework that utilizes continuous-time Markov chains to solve the SB problem in a high-dimensional discrete state space.
Our approach extends Iterative Markovian Fitting to discrete domains, and we have proved its convergence to the SB.
Furthermore, we adapt our framework for the graph transformation, and show that our design choice of underlying dynamics characterized by independent modifications of nodes and edges can be interpreted as the entropy-regularized version of optimal transport with a cost function described by the graph edit distance.
To demonstrate the effectiveness of our framework, we have applied DDSBM to molecular optimization in the field of chemistry.
Experimental results demonstrate that DDSBM effectively optimizes molecules' property-of-interest with minimal graph transformation, successfully retaining other features.","['SchrÃ¶dinger Bridge', 'Discrete Diffusion Model', 'Molecular Optimization']",,https://openreview.net/pdf?id=p0KYWGpAHB,accept
https://openreview.net/forum?id=jzX3SrFSci,Language Models for Text-guided Protein Evolution,"['Zhanghan Ni', 'Shengchao Liu', 'Hongyu Guo', 'Anima Anandkumar']","Language models have demonstrated efficacy in protein design by capturing the distribution of amino acid sequences and structures. To advance protein representation learning, biomedical textual description has been integrated as an additional modality, complementing existing sequence and structure information. The textual modality is crucial as it provides insights into detailed molecular functions and cellular contexts in which proteins operate. Existing deep learning methods have built foundation models based on this modality, aiming for challenging protein design tasks, including text-to-protein generation and text-guided protein editing. Meanwhile, the capability of utilizing such multiple modalities to handle natural protein evolution remains an open question. In this work, we introduce two tasks: text-guided point mutation and text-guided Enzyme Commission number switching. These tasks enable a preliminary exploration of the boundaries of utilizing a multimodal foundation model to understand protein evolution process. We assess existing language models on novel protein evolution tasks: text-guided point mutation and EC number switching.  Our results show that structure-based models outperform sequence-based ones by 24\% in structure-oriented evolution tasks, despite exhibiting significant biases. We also find that models using free-form text more effectively design enzyme functions, achieving a 30.06\% closer alignment to target functions by integrating evolutionary context.","['Protein Design', 'Protein Evolution', 'Protein Representation Learning', 'Multimodal Learning', 'Large Language Models']",,https://openreview.net/pdf?id=jzX3SrFSci,accept
https://openreview.net/forum?id=e3JoUmFDzN,Generative Flows on Synthetic Pathway for Drug Design,"['Seonghwan Seo', 'Minsu Kim', 'Tony Shen', 'Martin Ester', 'Jinkyoo Park', 'Sungsoo Ahn', 'Woo Youn Kim']","Generative models in drug discovery have recently gained attention as efficient alternatives to brute-force virtual screening. However, most existing models do not account for synthesizability, limiting their practical use in real-world scenarios. In this paper, we propose RxnFlow, which sequentially assembles molecules using predefined molecular building blocks and chemical reaction templates to constrain the synthetic chemical pathway. We then train on this sequential generating process with the objective of generative flow networks (GFlowNets) to generate both highly rewarded and diverse molecules. To mitigate the large action space of synthetic pathways in GFlowNets, we implement a novel action space subsampling method. This enables RxnFlow to learn generative flows over extensive action spaces comprising combinations of 1.2 million building blocks and 71 reaction templates without significant computational overhead. Additionally, RxnFlow can employ modified or expanded action spaces for generation without retraining, allowing for the introduction of additional objectives or the incorporation of newly discovered building blocks. We experimentally demonstrate that RxnFlow outperforms existing reaction-based and fragment-based models in pocket-specific optimization across various target pockets. Furthermore, RxnFlow achieves state-of-the-art performance on CrossDocked2020 for pocket-conditional generation, with an average Vina score of â€“8.85 kcal/mol and 34.8% synthesizability.","['GFlowNet', 'synthesizability', 'structure-based drug design', 'molecule optimization']",,https://openreview.net/pdf?id=e3JoUmFDzN,accept
https://openreview.net/forum?id=K4wifbtx95,Modeling Complex System Dynamics with Flow Matching Across Time and Conditions,"['Martin Rohbeck', 'Charlotte Bunne', 'Edward De Brouwer', 'Jan-Christian Huetter', 'Anne Biton', 'Kelvin Y. Chen', 'Aviv Regev', 'Romain Lopez']","Modeling the dynamics of complex real-world systems from temporal snapshot data is crucial for understanding phenomena such as gene regulation, climate change, and financial market fluctuations. Researchers have recently proposed a few methods based either on the Schroedinger Bridge or Flow Matching to tackle this problem, but these approaches remain limited in their ability to effectively combine data from multiple time points and different experimental settings. This integration is essential in real-world scenarios where observations from certain combinations of time points and experimental conditions are missing, either because of experimental costs or sensory failure. To address this challenge, we propose a novel method named Multi-Marginal Flow Matching (MMFM). MMFM first constructs a flow using smooth spline-based interpolation across time points and conditions and regresses it with a neural network using the classifier-free guided Flow Matching framework. This framework allows for the sharing of contextual information about the dynamics across multiple trajectories. We demonstrate the effectiveness of our method on both synthetic and real-world datasets, including a recent single-cell genomics data set with around a hundred chemical perturbations across time points. Our results show that MMFM significantly outperforms existing methods at imputing data at missing time points.","['Flow Matching', 'dynamical systems']",,https://openreview.net/pdf?id=K4wifbtx95,accept
https://openreview.net/forum?id=AAiB5qYyHw,JAMUN: Transferable Molecular Conformational Ensemble Generation with Walk-Jump Sampling,"['Ameya Daigavane', 'Bodhi P. Vani', 'Saeed Saremi', 'Joshua A Rackers', 'Joseph Kleinhenz']","Conformational ensembles of protein structures are immensely important both to understanding protein function, and for drug discovery in novel modalities such as cryptic pockets. Current techniques for sampling ensembles are computationally inefficient, or do not transfer to systems outside their training data. We present walk-Jump Accelerated Molecular ensembles with Universal Noise (JAMUN), a step towards the goal of efficiently sampling the Boltzmann distribution of arbitrary proteins. By extending Walk-Jump Sampling to point clouds, JAMUN enables ensemble generation at orders of magnitude faster rates than traditional molecular dynamics or state-of-the-art generators. Further, JAMUN is able to predict the stable basins of small peptides that were not seen during training.","['Proteins', 'Ensembles', 'Conformations', 'Thermodynamics', 'Generative']",,https://openreview.net/pdf?id=AAiB5qYyHw,accept
https://openreview.net/forum?id=LfBIZzQsvg,SmileyLlama: Modifying Large Language Models \\for Directed Chemical Space Exploration,"['Joe Cavanagh', 'Kunyang Sun', 'Andrew Gritsevskiy', 'Dorian Bagni', 'Teresa Head-Gordon', 'Thomas D. Bannister']","Here we show that a Large Language Model (LLM) can serve as a foundation model for a Chemical Language Model (CLM) which performs at or above the level of CLMs trained solely on chemical SMILES string data. Using supervised fine-tuning (SFT) and direct preference optimization (DPO) on the open-source Llama LLM, we demonstrate that we can train an LLM to respond to prompts such as generating molecules with properties of interest to drug development. This overall framework allows an LLM to not just be a chatbot client for chemistry and materials tasks, but can be adapted to speak more directly as a CLM which can generate molecules with user-specified properties.","['LLM', 'molecule design', 'drug design', 'chemical language model', 'SMILES', 'chemistry']",,https://openreview.net/pdf?id=LfBIZzQsvg,accept
https://openreview.net/forum?id=A9FlQMKxJ4,Geometry-text Multi-modal Foundation Model for Reactivity-oriented Molecule Editing,"['Haorui Li', 'Shengchao Liu', 'Hongyu Guo', 'Anima Anandkumar']","Recent breakthroughs in foundation models have revolutionized the science domain with their promising generalization performance to solve challenging open questions. In chemistry and biology, the textual data enriches comprehensive knowledge about the molecule's functionalities, thus serving as a complementary modality to the chemical structures. However, existing multi-modal foundation models mainly focus on 2D topology rather than 3D geometry. To handle this issue, we construct a large-scale 3D structure-text dataset with conformations calculated by semi-empirical quantum methods. Then we propose MoleculeSTM-3D, a geometry-text multi-modal foundation model to align the two modalities through contrastive learning. For downstream, we apply MoleculeSTM-3D to the reactivity-oriented molecule editing task. Our empirical results demonstrate that it achieves a 9.00% higher editing success rate and significantly reduces invalid molecule generation by 10.07% compared to baseline methods. Such preliminary results reveal the potential of utilizing MoleculeSTM-3D for solving more challenging tasks.","['Multi-modal Foundation Model', 'Drug Discovery', 'Contrastive Learning', '3D GNN', 'LLM']",,https://openreview.net/pdf?id=A9FlQMKxJ4,accept
https://openreview.net/forum?id=GvgQIIBdfe,Derivative-Free Guidance in Continuous and Discrete Diffusion Models with Soft Value-Based Decoding,"['Xiner Li', 'Yulai Zhao', 'Chenyu Wang', 'Gabriele Scalia', 'GÃ¶kcen Eraslan', 'Surag Nair', 'Tommaso Biancalani', 'Shuiwang Ji', 'Aviv Regev', 'Sergey Levine', 'Masatoshi Uehara']","Diffusion models excel at capturing the natural design spaces of images, molecules, and biological sequences of DNA, RNA, and proteins. However, for many applications from biological research to biotherapeutic discovery, rather than merely generating designs that are natural, we aim to optimize downstream reward functions while preserving the naturalness of these design spaces. Existing methods for achieving this goal often require ``differentiable'' proxy models (e.g., classifier guidance) or computationally-expensive fine-tuning of diffusion models (e.g., classifier-free guidance, RL-based fine-tuning). Here, we propose a new method, known as SVDD, to address these challenges. SVDD is an iterative sampling method that integrates soft value functions, which looks ahead to how intermediate noisy states lead to high rewards in the future, into the standard inference procedure of pre-trained diffusion models. Notably, our approach avoids fine-tuning generative models and eliminates the need to construct differentiable models. This enables us to (1) directly utilize non-differentiable features/reward feedback, commonly used in many scientific domains, and (2) apply our method to recent discrete diffusion models in a principled way. Finally, we demonstrate the effectiveness of our algorithm across several domains, including image generation, molecule generation (optimization of docking scores, QED, SA), and DNA/RNA generation (optimization of activity levels).","['Diffusion models', 'Reinforcement learning', 'AI for science']",,https://openreview.net/pdf?id=GvgQIIBdfe,accept
https://openreview.net/forum?id=L1MyyRCAjX,Natural Language Prompts Guide the Design of Novel Functional Protein Sequences,"['Niksa Praljak', 'Hugh Yeh', 'Miranda Moore', 'Michael Socolich', 'Rama Ranganathan', 'Andrew Ferguson']","The advent of natural language interaction with machines has ushered in new innovations in text-guided generation of images, audio, video, and more. In this arena, we introduce Biological Multi-Modal Model (BioM3), as a novel framework for designing functional proteins via natural language prompts. This framework integrates natural language with protein design through a three-stage process: aligning protein and text representations in a joint embedding space learned using contrastive learning, refinement of the text embeddings, and conditional generation of protein sequences via a discrete autoregressive diffusion model. BioM3 synthesizes protein sequences with detailed descriptions of the protein structure, lineage, and function from text annotations to enable the conditional generation of novel sequences with desired attributes through natural language prompts. We present in silico validation of the model predictions for subcellular localization prediction, reaction classification, remote homology detection, scaffold in-painting, and structural plausibility, and in vivo and in vitro experimental tests of natural language prompt-designed synthetic analogs of Src-homology 3 (SH3) domain proteins that mediate signaling in the Sho1 osmotic stress response pathway in baker's yeast. BioM3 possesses state-of-the-art performance in zero-shot prediction and homology detection tasks, and generates proteins with native-like tertiary folds and wild-type levels of experimentally assayed function.","['Multimodality', 'Deep Generative Models', 'Protein Design']",,https://openreview.net/pdf?id=L1MyyRCAjX,accept
https://openreview.net/forum?id=Y61fBtTfih,A Foundational Multi-Modal Knowledge Graph for Pancreatic Cancer Drug Effects Prediction,"['Jingwen Hui', 'Shengchao Liu', 'Xiaohua Huang', 'Anima Anandkumar']","AI-assisted drug discovery has revolutionized healthcare by accelerating virtual screening methods as compared to traditional processes. Many advanced AI models have been developed to predict and generate drug candidates, with potential applications across various diseases. However, challenges still remain in applying AI models in clinical settings. These include the lack of heterogeneity and insufficient consideration of patient-specific treatment plans. To mitigate these challenges, we propose PanRX, a cell-line-specific pancreatic cancer drug effect model using multi-modal knowledge graphs. It aims at achieving a personalized drug discovery framework by incorporating rich genetic and chemical information. We first construct a multi-modal knowledge graph dataset PanCan-DrugsGenes. It extracts textual genetic information from NCBI, mutation status from the Genomics of Drug Sensitivity in Cancer (GDSC) dataset, textual descriptions of drugs from PubChem, and chemical geometry from the PM6 dataset. Then, PanRX utilizes a geometric model to learn chemical conformation, a language model to learn textual description, and a graph neural network to fuse all information and predict the target drug effects. We verify the effectiveness of PanRX by achieving the generalization performance with very low MSE (< 0.0000) and MAE (0.0009). This work emphasizes the potential of merging knowledge graphs and deep learning in the fields of genomics and medicine, enriching the intersection of human biological expertise and AI in drug discovery and design tasks.","['Foundational Model', 'Drug Discovery', 'Computational Biology', 'Personalized Medicine']",,https://openreview.net/pdf?id=Y61fBtTfih,accept
https://openreview.net/forum?id=30EakJqzF0,Foundational Model-aided Automatic High-throughput Drug Screening Using Self-controlled Cohort Study,"['Shenbo Xu', 'Raluca Cobzaru', 'Stan Finkelstein', 'Roy Welsch', 'Kenney Ng']","The process of developing new drugs, from initial discovery to obtaining regulatory approval, has historically been neither cost-efficient, expeditious, nor free from risk. The growing availability of large-scale observational healthcare databases, combined with the rise of foundational models, offer an unparalleled opportunity to enable automatic high-throughput drug screening for both repurposing and pharmacovigilance. In this work, we present a general workflow for automatic high-throughput drug screening which estimates the association between various drug exposures and disease outcomes. We provide frameworks for parsing the accurate exposure length for each prescription from clinical texts and removing confounding relationships between drugs and diseases using bioinformatic mapping and foundational models. Using a self-controlled cohort study design, we tested the intention-to-treat association between 3,444 medications and 276 diseases across 6.6 million UK patients from the Clinical Practice Research Datalink (CPRD). 
Our analysis revealed 16,901 drug-disease pairs with significant risk reduction, indicating candidates for repurposing, as well as 11,089 pairs with significant risk increase which raise drug safety concerns. 
Our data-driven, nonparametric, hypothesis-generating, and automatic approach demonstrates the potential of foundational models in drug discovery and provides a scalable framework for drug repurposing that can be extended to other observational medical databases.","['drug screening', 'drug repurposing', 'foundational model', 'self-controlled cohort study', 'incidence rate ratio']",,https://openreview.net/pdf?id=30EakJqzF0,accept
https://openreview.net/forum?id=BDISzo0dZi,An Efficient Tokenization for Molecular Language Models,"['Seojin Kim', 'Jaehyun Nam', 'Jinwoo Shin']","Recently, molecular language models have shown great potential in various chemical applications, e.g., drug-discovery. These models adapt auto-regressive language models to molecular data by considering molecules as sequences of atoms, where each atom is mapped to individual tokens of the language models. However, such atom-level tokenizations limit the models' ability to capture the global structural context of molecules. To tackle this issue, we propose a novel molecular language model, coined Context-Aware Molecular T5 (CAMT5). Inspired by the importance of the substructure-level contexts, e.g., ring systems, in understanding molecules, we introduce substructure-level tokenization for molecular language models. Specifically, we construct a tree structure for each molecule whose nodes correspond to important substructures, i.e., motifs. Then, we train our CAMT5 by considering a molecule as a sequence of motif tokens, whose order is determined by a tree-search algorithm. Under the proposed motif token space, one can incorporate chemical context with a significantly shorter token length (than atom-level tokenizations), which is useful for mitigating the issues during the auto-regressive molecular generation, e.g., error propagation. In addition, CAMT5 guarantees to generate a valid molecule with non-degeneracy, i.e., no ambiguity in the meaning of each token, which is 
also overlooked in previous models. Extensive experiments demonstrate the effectiveness of CAMT5 in the text-to-molecule generation task. Finally, we also propose a simple strategy of ensemble that can aggregate the outputs of molecular language models of different tokenizations, e.g., SMILES, SELFIES and ours, further boosting the quality of the generated molecules.","['drug discovery', 'text-to-molecule generation', 'molecular language models']",,https://openreview.net/pdf?id=BDISzo0dZi,accept
https://openreview.net/forum?id=2xmqiebwgY,TrialDura: Hierarchical Attention Transformer for Interpretable Clinical Trial Duration Prediction,"['Ling Yue', 'Sixue Xing', 'Jonathan Li', 'MD Zabirul Islam', 'Bolun Xia', 'Jintai Chen', 'Tianfan Fu']","The clinical trial process, a critical phase in drug development, is essential for developing new treatments. The primary goal of interventional clinical trials is to evaluate the safety and efficacy of drug-based treatments for specific diseases. However, these trials are often lengthy, labor-intensive, and expensive. The duration of a clinical trial significantly impacts overall costs, making efficient timeline management crucial for controlling budgets and ensuring the economic feasibility of research.
To address this issue, We propose TrialDura, a multimodal and interpretable deep learning method that estimates the duration of clinical trials using multimodal data, including disease names, drug molecules, trial phases, and eligibility criteria. Then, we encode them into Bio-BERT embeddings specifically tuned for biomedical contexts to provide a deeper and more relevant semantic understanding of clinical trial data. Finally, the modelâ€™s hierarchical attention mechanism connects all of the embeddings to capture their interactions and predict clinical trial duration. Our proposed model demonstrated superior performance with a mean absolute error (MAE) of 1.04 years and a root mean square error (RMSE) of 1.39 years compared to the other models, indicating more accurate clinical trial duration prediction. 
Publicly available code can be found at: https://github.com/LeoYML/TrialDura .","['Clinical Trial', 'Drug Development', 'Drug Discovery', 'Interpretability', 'Deep Learning']",,https://openreview.net/pdf?id=2xmqiebwgY,accept
https://openreview.net/forum?id=15AkNhFX1R,Structure Language Models for Protein Conformation Generation,"['Jiarui Lu', 'Xiaoyin Chen', 'Stephen Zhewen Lu', 'Chence Shi', 'Hongyu Guo', 'Yoshua Bengio', 'Jian Tang']","Proteins adopt multiple structural conformations to perform their diverse biological functions, and understanding these conformations is crucial for advancing drug discovery. Traditional physics-based simulation methods often struggle with sampling equilibrium conformations and are computationally expensive. Recently, deep generative models have shown promise in generating protein conformations as a more efficient alternative. However, these methods predominantly rely on the diffusion process within a 3D geometric space, which typically centers around the vicinity of metastable states and is often inefficient in terms of runtime. In this paper, we introduce Structure Language Modeling (SLM) as a novel framework for efficient protein conformation generation. Specifically, the protein structures are first encoded into a compact latent space using a discrete variational auto-encoder, followed by conditional language modeling that effectively captures sequence-specific conformation distributions.  This enables a more efficient and interpretable exploration of diverse ensemble modes compared to existing methods. Based on this general framework, we instantiate SLM with various popular LM architectures as well as proposing the ESMDiff, a novel BERT-like structure language model fine-tuned from ESM3 with masked diffusion. We verify our approach in various scenarios, including the equilibrium dynamics of BPTI, conformational change pairs, and intrinsically disordered proteins. SLM provides a highly efficient solution, offering a 20-100x speedup than existing methods in generating diverse conformations, shedding light on promising avenues for future research.","['protein', 'conformation generation', 'conformation sampling', 'generative models', 'language models', 'diffusion models']",,https://openreview.net/pdf?id=15AkNhFX1R,accept
https://openreview.net/forum?id=bg0CMOFucM,SMORE-DRL: Scalable Multi-Objective Robust and Efficient Deep Reinforcement Learning for Molecular Optimization,"['Aws Al Jumaily', 'Nicholas Aksamit', 'Yage Zhang', 'Mohammad Sajjad Ghaemi', 'Jinqiang Hou', 'Hsu Kiang Ooi', 'Yifeng Li']","The adoption of AI techniques within the domain of drug design provides an opportunity of systematic and efficient exploration of the vast chemical search space. In recent years, advancements in this domain have been driven by AI frameworks, including deep reinforcement learning (DRL). However, the scalability and performance of existing methodologies are constrained by prolonged training periods and inefficient sample data utilization. Furthermore, generalization capabilities of these models have not been fully investigated. To overcome these limitations, we take a multi-objective optimization perspective and introduce SMORE-DRL, a fragment and transformer-based multi-objective DRL architecture for the optimization of molecules across multiple pharmacological properties, including binding affinity to a cancer protein target. Our approach involves pretraining a transformer-encoder model on molecules encoded by a novel hybrid fragment-SMILES representation method. Fine-tuning is performed through a novel gradient-alignment-based DRL, where lead molecules are optimized by selecting and replacing their fragments with alternatives from a fragment dictionary, ultimately resulting in more desirable drug candidates. Our findings indicate that SMORE-DRL is superior to current DRL models for lead optimization in terms of quality, efficiency, scalability, and robustness. Furthermore, SMORE-DRL demonstrates the capability of generalizing its optimization process to lead molecules that are not present during the pretraining or fine-tuning phases.","['Drug Design', 'Molecular Optimization', 'Deep Reinforcement Learning', 'Multi-Objective Optimization', 'Transformer', 'Scalable']",,https://openreview.net/pdf?id=bg0CMOFucM,accept
https://openreview.net/forum?id=x9uqoYNUbZ,Scaling Dense Representations for Single Cell Gene Expression with Transcriptome-Scale Context,"['Nicholas Ho', 'Caleb Ellington', 'Jinyu Hou', 'Sohan Addagudi', 'Shentong Mo', 'Tianhua Tao', 'Dian Li', 'Yonghao Zhuang', 'Hongyi Wang', 'Xingyi Cheng', 'Le Song', 'Eric P. Xing']","Developing a unified model of cellular systems is a canonical challenge in biology. Recently, a wealth of public single-cell RNA sequencing data as well as rapid scaling of self-supervised learning methods have provided new avenues to address this longstanding challenge. However, rapid parameter scaling has been essential to the success of large language models in text and images, while similar scaling has not been attempted with Transformer architectures for cellular modeling. To produce accurate, transferable, and biologically meaningful representations of cellular systems, we develop AIDO.Cell, which is a pretrained module for cell gene expression data in an AI-driven Digital Organism [1]. AIDO.Cell contains a series of 3M, 10M, 100M, and 650M parameter encoder-only dense Transformer models pre-trained on 50 million human cells from diverse tissues using a read-depth-aware masked gene expression pretraining objective. Unlike previous models, AIDO.CELL is capable of handling the entire human transcriptome as input without truncation or sampling tricks, thus learning accurate and general representations of the human cellâ€™s entire transcriptional context. This pretraining with a longer context was enabled through FlashAttention-2, mixed precision, and large-scale distributed systems training. AIDO.CELL (100M) achieves state-of-the-art results in tasks such as zero-shot clustering, cell-type classification, and perturbation modeling. Our findings reveal interesting loss scaling behaviors as we increase AIDO.CELLâ€™s parameters from 3M to 650M, providing insights for future directions in single-cell modeling. Models and code are available through ModelGenerator in https://github.com/genbio-ai/AIDO and on Hugging Face.","['Single Cell', 'Genomics', 'Foundation Models', 'Pretraining', 'Transfer Learning']",,https://openreview.net/pdf?id=x9uqoYNUbZ,accept
https://openreview.net/forum?id=pCz6OTCodu,PertEval-scFM: Benchmarking Single-Cell Foundation Models for Perturbation Effect Prediction,"['Aaron Wenteler', 'Martina Occhetta', 'Nikhil Branson', 'Magdalena Huebner', 'William Dee', 'Victor Curean', 'William Connell', 'Siu Pui Chung', 'Yasha Ektefaie', 'Amaya Gallagher-Syed', 'CÃ©sar Miguel Valdez CÃ³rdova']","_In silico_ modeling of transcriptional responses to perturbations is crucial for advancing our understanding of cellular processes and disease mechanisms. We present PertEval-scFM, a standardized framework designed to evaluate models for perturbation effect prediction. We apply PertEval-scFM to benchmark zero-shot single-cell foundation model (scFM) embeddings against simpler baseline models to assess whether these contextualized representations enhance perturbation effect prediction. Our results show that scFM embeddings do not provide consistent improvements over baseline models, especially under distribution shift. Additionally, all models struggle with predicting strong or atypical perturbation effects. Overall, this study provides a systematic evaluation of zero-shot scFM embeddings for perturbation effect prediction, highlighting the challenges of this task and revealing the limitations of current-generation scFMs. Our findings underscore the need for specialized models and high-quality datasets that capture a broader range of cellular states. Source code and documentation can be found at: https://github.com/aaronwtr/PertEval.","['foundation models', 'benchmark', 'single-cell biology', 'perturbation effect prediction']",,https://openreview.net/pdf?id=pCz6OTCodu,accept
https://openreview.net/forum?id=TN6GpIkXDB,ProtPainter: Draw or Drag Protein via Topology-guided Diffusion,"['Zhengxi Lu', 'Shizhuo Cheng', 'Yuru Jiang', 'Yan Zhang', 'Min Zhang']","Recent advances in protein backbone generation have achieved promising results under structural, functional, or physical constraints. However, existing methods lack the flexibility for precise topology control, limiting navigation of the backbone space. We present $\textbf{ProtPainter}$, a diffusion-based approach for generating protein backbones conditioned on 3D curves. ProtPainter follows a two-stage process: curve-based sketching and sketch-guided backbone generation. For the first stage, we propose $\textbf{CurveEncoder}$, which predicts secondary structure annotations from a curve to parametrize sketch generation. For the second stage, the sketch guides the generative process in Denoising Diffusion Probabilistic Modeling (DDPM) to generate backbones. During the process, we further introduce a fusion scheduling scheme, Helix-Gating, to control the scaling factors. To evaluate, we propose the first benchmark for topology-conditioned protein generation, introducing Protein Restoration Task and a new metric, self-consistency Topology Fitness (scTF). Experiments demonstrate ProtPainter's ability to generate topology-fit (scTF $>$ 0.8) and designable (scTM $>$ 0.5) backbones, with drawing and dragging tasks showcasing its flexibility and versatility.","['Protein Backbone Generation', 'Conditional Diffusion', 'Topology Representation', 'Protein Editing']",,https://openreview.net/pdf?id=TN6GpIkXDB,accept
https://openreview.net/forum?id=cFHjEo2oCd,Chain-of-thoughts for molecular understanding,"['Yunhui Jang', 'Jaehyung Kim', 'Sungsoo Ahn']","The adaptation of large language models (LLMs) to chemistry have shown promising performance in molecular understanding tasks, such as generating a text description from a molecule. However, proper reasoning based on molecular structural information remains a significant challenge, e.g., even advanced LLMs such as GPT-4o struggle to identify functional groups which are crucial for inferring the molecular property of interest. To address this limitation, we propose \Algname, a structure-aware chain-of-thought (CoT) that enhances LLMsâ€™ understanding of molecular structures by explicitly injecting the key structural features of molecules. Moreover, we introduce two fine-tuning frameworks for adapting the existing LLMs to use our \Algname. Our experiments demonstrate that incorporating \Algname with our fine-tuning frameworks leads to consistent improvements in both molecular understanding tasks.","['chain-of-thought', 'molecular understanding', 'large language model']",,https://openreview.net/pdf?id=cFHjEo2oCd,accept
https://openreview.net/forum?id=jBkFe5zfPa,Weighted Diversified Sampling for Efficient Data-Driven Single-Cell Gene-Gene Interaction Discovery,"['Yifan Wu', 'Yuntao Yang', 'Zirui Liu', 'Zhao Li', 'Khushbu Pahwa', 'Rongbin Li', 'Wenjin Zheng', 'Xia Hu', 'Zhaozhuo Xu']","Gene-gene interactions play a crucial role in the manifestation of complex human diseases. Uncovering significant gene-gene interactions is a challenging task. Here, we present an innovative approach utilizing data-driven computational tools, leveraging an advanced Transformer model, to unearth noteworthy gene-gene interactions. Despite the efficacy of Transformer models, their parameter intensity presents a bottleneck in data ingestion, hindering data efficiency.  To mitigate this, we introduce a novel weighted diversified sampling algorithm. This algorithm computes the diversity score of each data sample in just two passes of the dataset, facilitating efficient subset generation for interaction discovery. Our extensive experimentation demonstrates that by sampling a mere 1% of the single-cell dataset, we achieve performance comparable to that of utilizing the entire dataset.","['Gene-gene interaction', 'sampling']",,https://openreview.net/pdf?id=jBkFe5zfPa,accept
https://openreview.net/forum?id=XycdKET8tX,Evaluating synergies among generative design models for multi-objective optimization of drug-like proteins,"['Jung-Eun Shin', 'Nathan J Rollins', 'Jordan M Anderson', 'Grace Carey', 'Allison Colthart', 'Thomas Hopf', 'Ivan Mascanfroni', 'Jyothsna Visweswaraiah', 'Yi Xing', 'Kevin L. Otipoby', 'Nathan Higginson-Scott', 'Ryan Peckner']","In recent years, the field of AI for protein design has made tremendous advances in the generation of high quality proteins. Much of the generative focus has been on producing de novo sequences and structures or in producing large libraries to screening for novel function, thereby leading to the discovery of a starting protein candidate. The downstream pipeline for drug discovery for engineering the protein candidate into a suitable therapeutic, however, has been much less a focus for generative protein design, especially for non-antibody biologics. For a protein to be a suitable therapeutic, it must not only have the desired function at a therapeutically relevant efficacy, it must also be manufacturable, safe, and non-immunogenic. Multi-objective optimization methods are uniquely suited to providing a one-pot model to simultaneously optimize for function and all desired drug-like properties. In this work we present individual models for the generation of fit, functional protein sequences and the prediction of drug-like properties, as well as generative models that harness the totality and subsets of these individual models in generating therapeutically developable proteins. We explore the synergy of these machine learning methods in modular generative multi-objective optimization models by comparing training data, generative model architectures, generation methods, and implementation of various developability constraints. By generating sequences from these modular multi-objective optimization models and experimentally screening proteins derived from IdeS (immunoglobulin degrading protease) for stability and function, we demonstrate that our models are able to consistently generate highly fit proteins that have been optimized to possess drug-like properties in silico.","['AI protein design', 'Multi-objective optimization', 'Evolutionary sequence models', 'Unsupervised protein sequence models', 'Immunoglobulin-degrading proteases']",,https://openreview.net/pdf?id=XycdKET8tX,accept
https://openreview.net/forum?id=y1VLghM9Xa,AptaBLE: A Deep Learning Platform for SELEX Optimization,"['Sawan Patel', 'Keith Fraser', 'Zhangzhi Peng', 'Adam D. Friedman', 'Owen Yao', 'Pranam Chatterjee', 'Sherwood Yao']","Aptamers are short, single-stranded DNA or RNA sequences that bind to specific targets such as proteins, small molecules, and cells, with high affinity and specificity. Aptamers can serve as molecular recognition elements, making them valuable for therapeutic applications, diagnostics, and targeted drug delivery alternatives to antibodies. Typically developed through in vitro selection, also known as the Systematic Evolution of Ligands by EXponential enrichment (SELEX), aptamer selection imposes multiple technical and resource constraints, such as the need to impose selective pressure intentionally and precisely, as well as the time (oftentimes months) and reagent costs to move from initial library to characterized aptamer. To overcome these limitations, we have developed AptaBLE (Aptamer Binding LanguagE): a large language model capable of predicting aptamer-protein interactions and generating novel aptamer sequences against diverse protein targets. Herein we demonstrate how AptaBLE leverages fused embeddings to score aptamer-protein binding in a structure-agnostic fashion. We report on performance gains that can be realized via AptaBLE when compared to other deep learning methods. Lastly, we highlight aptamer discovery and optimization workflows which are made possible by AptaBLE, including its capacity to produce novel aptamers with favorable binding profiles.","['aptamer', 'LLM', 'fusion', 'protein', 'binding', 'tool', 'function', 'SELEX', 'de-novo', 'optimization', 'generation']",,https://openreview.net/pdf?id=y1VLghM9Xa,accept
https://openreview.net/forum?id=XVu9AhPdfZ,"Machine learning enables engineering of potent, specific, and therapeutically developable proteases","['Jung-Eun Shin', 'Nathan J Rollins', 'Purvi Mande', 'Jordan M Anderson', 'Allison Colthart', 'Soumya Bengeri', 'Emily Hoyt', 'Alex Pellerin', 'Ivan Mascanfroni', 'Jyothsna Visweswaraiah', 'Yi Xing', 'Kevin L. Otipoby', 'Nathan Higginson-Scott', 'Ryan Peckner']","The development of biologic therapeutics is a labor-intensive, time-consuming, and expensive process. From the discovery of a protein with the desired function to optimizing the protein for manufacturability and removing immunogenicity, each step requires many iterations and large screening efforts. In order to accelerate this process, efforts have been increasingly focused on methods for protein engineering and design, both experimentally and computationally. Here we show that the careful application of machine learning methods in combination with structure-based and data-driven rational design allows for successful engineering of proteases with desired target specificity, low immunogenicity, and potent cleavage potential. We present two case studies describing the impact of machine learning in the development of two potential therapeutics. First, we apply machine learning models to optimize an IgG-degrading protease for drug-like properties while minimizing immunogenicity and other liabilities and maintaining therapeutic activity. Second, we leverage both machine learning and rational design methods to engineer an IgE-degrading protease with high specificity and activity. These results demonstrate the power of applying machine learning methods to accelerate the discovery and development of protein therapeutics.","['Drug design', 'Immunogenicity', 'T cell epitope prediction', 'Rational design', 'Enzyme therapeutics']",,https://openreview.net/pdf?id=XVu9AhPdfZ,accept
https://openreview.net/forum?id=xHxgopB4pX,Molphenix: A Multimodal Foundation Model for PhenoMolecular Retrieval,"['Philip Fradkin', 'Puria Azadi Moghadam', 'Karush Suri', 'Frederik Wenkel', 'Maciej Sypetkowski', 'Dominique Beaini']","Predicting molecular impact on cellular function is a core challenge in therapeutic design. 
Phenomic experiments, designed to capture cellular morphology, utilize microscopy based techniques and demonstrate a high throughput solution for uncovering molecular impact on the cell. 
In this work, we learn a joint latent space between molecular structures and microscopy phenomic experiments, aligning paired samples with contrastive learning. 
Specifically, we study the problem of Contrastive PhenoMolecular Retrieval, which consists of zero-shot molecular structure identification conditioned on phenomic experiments. 
We assess challenges in multi-modal learning of phenomics and molecular modalities such as experimental batch effect, inactive molecule perturbations, and encoding perturbation concentration.
We demonstrate improved multi-modal learner retrieval through (1) a uni-modal pre-trained phenomics model, (2) a novel inter sample similarity aware loss, and (3) models conditioned on a representation of molecular concentration.
Following this recipe, we propose MolPhenix, a molecular phenomics model.
MolPhenix leverages a pre-trained phenomics model to demonstrate significant performance gains across perturbation concentrations, molecular scaffolds, and activity thresholds. 
In particular, we demonstrate an 8.1$\times$ improvement in zero shot molecular retrieval of active molecules over the previous state-of-the-art, reaching 77.33% in top-1% accuracy. 
These results open the door for machine learning to be applied in virtual phenomics screening, which can significantly benefit drug discovery applications.","['Multi-Modality', 'Contrastive Learning', 'CLIP', 'Cell Morphology', 'Molecules', 'Molecular Retrieval', 'Zero-Shot Learning', 'Cell-Painting']",,https://openreview.net/pdf?id=xHxgopB4pX,accept
https://openreview.net/forum?id=otpO8k1fua,Beyond Sequence: Impact of Geometric Context for RNA Property Prediction,"['Junjie Xu', 'Artem Moskalev', 'Tommaso Mansi', 'Mangal Prakash', 'Rui Liao']","Accurate prediction of RNA properties, such as stability and interactions, is crucial for advancing our understanding of biological processes and developing RNA-based therapeutics. RNA structures can be represented as 1D sequences, 2D topological graphs, or 3D all-atom models, each offering different insights into its function. Existing works predominantly focus on 1D sequence-based models, which overlook the geometric context provided by 2D and 3D geometries. This study presents the first systematic evaluation of incorporating explicit 2D and 3D geometric information into RNA property prediction, considering not only performance but also real-world challenges such as limited data availability, partial labeling, sequencing noise, and computational efficiency. To this end, we introduce a newly curated set of RNA datasets with enhanced 2D and 3D structural annotations, providing a resource for model evaluation on RNA data. Our findings reveal that models with explicit geometry encoding generally outperform sequence-based models, with an average prediction RMSE reduction of around 12% across all various RNA tasks and excelling in low-data and partial labeling regimes, underscoring the value of explicitly incorporating geometric context. On the other hand, geometry-unaware sequence-based models are more robust under sequencing noise but often require around 2-5x training data to match the performance of geometry-aware models. Our study offers further insights into the trade-offs between different RNA representations in practical applications and addresses a significant gap in evaluating deep learning models for RNA tasks.","['Geometric deep learning', 'Graph Neural Networks', 'GNNs', 'RNA property prediction', 'explicit structural information', 'curated datasets', 'benchmark models', 'noise and robustness', 'partial labeling', 'OOD generalization']",,https://openreview.net/pdf?id=otpO8k1fua,accept
https://openreview.net/forum?id=EiGerSsI5P,GFlowNet Pretraining with Inexpensive Rewards,"['Mohit Pandey', 'Gopeshh Subbaraj', 'Emmanuel Bengio']","Generative Flow Networks (GFlowNets), a class of generative models have recently emerged as a suitable framework for generating diverse and high-quality molecular structures by learning from unnormalized reward distributions. Previous works in this direction often restrict exploration by using predefined molecular fragments as building blocks, limiting the chemical space that can be accessed. In this work, we introduce Atomic GFlowNets (A-GFN), a foundational generative model leveraging individual atoms as building blocks to explore drug-like chemical space more comprehensively. We propose an unsupervised pre-training approach using offline drug-like molecule datasets, which conditions A-GFNs on inexpensive yet informative molecular descriptors such as drug-likeliness, topological polar surface area, and synthetic accessibility scores. These properties serve as proxy rewards, guiding A-GFNs towards regions of chemical space that exhibit desirable pharmacological properties. We further our method by implementing a goal-conditioned fine-tuning process, which adapts A-GFNs to optimize for specific target properties. In this work, we pretrain A-GFN on the ZINC15 offline dataset and employ robust evaluation metrics to show the effectiveness of our approach when compared to other relevant baseline methods in drug design.","['GFlowNet', 'Foundational Models', 'Small Molecules']",,https://openreview.net/pdf?id=EiGerSsI5P,accept
https://openreview.net/forum?id=aV9kP3qCrK,ML-driven design of 3â€™ untranslated regions for mRNA stability,"['Alyssa Kramer Morrow', 'Elise Duboscq Flynn', 'Emily Hoelzli', 'Ashley Thornal', 'Meimei Shan', 'Aniketh Janardhan Reddy', 'Gorkem Garipler', 'Rory Kirchner', 'Sophia Tabchouri', 'Ankit Gupta', 'Jean-Baptiste Michel', 'Uri Laserson']","Using mRNA as a therapeutic has received enormous attention in the last few years, but instability of the molecule remains a hurdle to achieving long-lasting therapeutic levels of protein expression. In this study, we describe our approach to designing stable mRNA molecules by combining machine learning-driven sequence design with high-throughput experimental assays. We developed a high-throughput massively parallel reporter assay (MPRA) that, in a single experiment, measures the half-life of tens of thousands of unique mRNA sequences containing designed 3â€™ untranslated regions (UTRs) that affect mRNA stability. Over multiple design-build-test iterations, we have accumulated mRNA stability measurements for 180,000 unique genomic and synthetic 3' UTRs, representing the largest such dataset of sequences. We trained highly-accurate machine learning models to map from 3â€™ UTR sequence to mRNA stability, and combined them with various ML design algorithms to guide the design of synthetic 3â€™ UTRs that increase mRNA stability in cell lines. Finally, we validated the function of several ML-designed 3â€™ UTRs in in vivo mouse models, resulting in up to 2-fold more protein production over time and 30â€“100-fold higher protein output at later time points compared to a commonly used benchmark. These results highlight the potential of ML-driven sequence design for mRNA therapeutics.","[""3' untranslated region"", 'mRNA design', 'ML driven design', 'synthetic biology', 'mRNA therapeutics']",,https://openreview.net/pdf?id=aV9kP3qCrK,accept
https://openreview.net/forum?id=Swe1i5VVaz,Modeling variable guide efficiency in pooled CRISPR screens with ContrastiveVI+,"['Ethan Weinberger', 'Tal Aschuach', 'Ryan Conrad']","Genetic screens mediated via CRISPR-Cas9 combined with high-content readouts have emerged as powerful tools for biological discovery. However, computational analyses of these screens come with additional challenges beyond those found with standard scRNA-seq analyses. For example, perturbation-induced variations of interest may be subtle and masked by other dominant source of variation shared with controls, and variable guide efficiency results in some cells not undergoing genetic perturbation despite expressing a guide RNA. While a number of methods have been developed to address the former problem by explicitly disentangling perturbation-induced variations from those shared with controls, less attention has been paid to the latter problem of noisy perturbation labels. To address this issue, here we propose ContrastiveVI+, a generative modeling framework that both disentangles perturbation-induced from non-perturbation-related variations while also inferring whether cells truly underwent genomic edits. Applied to three large-scale Perturb-seq datasets, we find that ContrastiveVI+ better recovers known perturbation-induced variations compared to previous methods while successfully identifying cells that escaped the functional consequences of guide RNA expression.","['scRNA-seq', 'perturb-seq', 'CRISPR', 'pooled genetic screens', 'variational inference']",,https://openreview.net/pdf?id=Swe1i5VVaz,accept
https://openreview.net/forum?id=4dQ5ChG6bB,3D Interaction Geometric Pre-training for Molecular Relational Learning,"['Namkyeong Lee', 'Yunhak Oh', 'Heewoong Noh', 'Gyoung S. Na', 'Tianfan Fu', 'Chanyoung Park']","Molecular Relational Learning (MRL) is a rapidly growing field that focuses on understanding the interaction dynamics between molecules, which is crucial for applications ranging from catalyst engineering to drug discovery. 
Despite recent progress, earlier MRL approaches are limited to using only the 2D topological structure of molecules, as obtaining the 3D interaction geometry remains prohibitively expensive.
This paper introduces a novel 3D geometric pre-training strategy for MRL (3DMRL) that incorporates a 3D virtual interaction environment, overcoming the limitations of costly traditional quantum mechanical calculation methods. 
With the constructed 3D virtual interaction environment, 3DMRL trains 2D MRL model to learn the overall 3D geometric information of molecular interaction through contrastive learning.
Moreover, fine-grained interaction between molecules is learned through force prediction loss, which is crucial in understanding the wide range of molecular interaction processes.
Extensive experiments on various tasks using real-world datasets, including out-of-distribution and extrapolation scenarios, demonstrate the effectiveness of 3DMRL, showing up to a 24.93\% improvement in performance across 40 tasks.","['Molecular Relational Learning', 'AI4Science', 'Geometric Deep Learning']",,https://openreview.net/pdf?id=4dQ5ChG6bB,accept
https://openreview.net/forum?id=Ioy8LCAyRj,Designing DNA With Tunable Regulatory Activity Using Discrete Diffusion,"['Anirban Sarkar', 'Ziqi Tang', 'Chris Z Zhao', 'Peter K Koo']","Engineering regulatory DNA sequences with precise activity levels in specific cell types hold immense potential for medicine and biotechnology. However, the vast combinatorial space of possible sequences and the complex regulatory grammars governing gene regulation have proven challenging for existing approaches. Supervised deep learning models that score sequences proposed by local search algorithms ignore the global structure of functional sequence space. While diffusion-based generative models have shown promise in learning these distributions, their application to regulatory DNA has been limited. Evaluating the quality of generated sequences also remains challenging due to a lack of a unified framework that characterizes key properties of regulatory DNA. Here we introduce DNA Discrete Diffusion (D3), a score entropy discrete diffusion model for DNA sequences, to conditionally generate regulatory sequences with targeted functional activity levels. We develop a comprehensive suite of evaluation metrics that assess the functional similarity, sequence similarity, and regulatory composition of generated sequences. Through benchmarking on three high-quality functional genomics datasets spanning human promoters and fly enhancers, we demonstrate that D3 outperforms existing methods in capturing the diversity of cis-regulatory grammars and generating sequences that more accurately reflect the properties of genomic regulatory DNA. Furthermore, we show that D3-generated sequences can effectively augment supervised models and improve their predictive performance, even in data-limited scenarios.","['DNA', 'DNN', 'Discrete Diffusion', 'Genomics']",,https://openreview.net/pdf?id=Ioy8LCAyRj,accept
https://openreview.net/forum?id=owwPzI9dyV,HELM: Hierarchical Encoding for mRNA Language Modeling,"['Mehdi Yazdani-Jahromi', 'Mangal Prakash', 'Tommaso Mansi', 'Artem Moskalev', 'Rui Liao']","Messenger RNA (mRNA) plays a crucial role in protein synthesis, with its codon structure directly impacting biological properties. While Language Models (LMs) have shown promise in analyzing biological sequences, existing approaches fail to account for the hierarchical nature of mRNA's codon structure. We introduce Hierarchical Encoding for mRNA Language Modeling (HELM), a novel pre-training strategy that incorporates codon-level hierarchical structure into language model training. HELM modulates the loss function based on codon synonymity, aligning the model's learning process with the biological reality of mRNA sequences. We evaluate HELM on diverse mRNA datasets and tasks, demonstrating that HELM outperforms standard language model pre-training as well as existing foundation model baselines on six diverse downstream property prediction tasks on average by around 8\%. Additionally, HELM enhances the generative capabilities of language model, producing diverse mRNA sequences that better align with the underlying true data distribution compared to  non-hierarchical baselines.","['Messenger RNA (mRNA)', 'Codon structure', 'Hierarchical modeling', 'Bio-language model', 'Property prediction', 'mRNA sequence generation']",,https://openreview.net/pdf?id=owwPzI9dyV,accept
https://openreview.net/forum?id=c7iR8y2m5d,Benchmarking Transcriptomics Foundation Models for Perturbation Analysis : one PCA still rules them all,"['Ihab Bendidi', 'Shawn T. Whitfield', 'Kian Kenyon-Dean', 'Hanene Ben Yedder', 'Yassir El Mesbahi', 'Emmanuel Noutahi', 'Alisandra Kaye Denton']","Understanding the relationships among genes, compounds, and their interactions in living organisms remains limited due to technological constraints and the complexity of biological data. Deep learning has shown promise in exploring these relationships using various data types. However, transcriptomics, which provides detailed insights into cellular states, is still underused due to its high noise levels and limited data availability. Recent advancements in transcriptomics sequencing provide new opportunities to uncover valuable insights, especially with the rise of many new foundation models for transcriptomics, yet no benchmark has been made to robustly evaluate the effectiveness of these rising models for perturbation analysis. This article presents a novel biologically motivated evaluation framework and a hierarchy of perturbation analysis tasks for comparing the performance of pretrained foundation models to each other and to more classical techniques of learning from transcriptomics data. We compile diverse public datasets from different sequencing techniques and cell lines to assess models performance. Our approach identifies scVI and PCA to be far better suited models for understanding biological perturbations in comparison to existing foundation models, especially in their application in real-world scenarios.","['Benchmark', 'Transcriptomics', 'Perturbation Analysis', 'Deep learning', 'Foundation Models', 'scVI', 'PCA', 'scGPT', 'UCE', 'Geneformer', 'CellPLM', 'Transfer learning', 'Zero Shot']",,https://openreview.net/pdf?id=c7iR8y2m5d,accept
https://openreview.net/forum?id=sajW6rHhyx,Harnessing Preference Optimisation in Protein LMs for Hit Maturation in Cell Therapy,"['Katarzyna Janocha', 'Annabel Ling', 'Alice Godson', 'Yulia Lampi', 'Simon Bornschein', 'Nils Yannick Hammerla']","Cell and immunotherapy offer transformative potential for treating diseases like cancer and autoimmune disorders by modulating the immune system. The development of these therapies is resource-intensive, with the majority of drug candidates failing to progress beyond laboratory testing. While recent advances in machine learning have revolutionised areas such as protein engineering, applications in immunotherapy remain limited due to the scarcity of large-scale, standardised datasets and the complexity of cellular systems. In this work, we address these challenges by leveraging a high-throughput experimental platform to generate data suitable for fine-tuning protein language models. We demonstrate how models fine-tuned using a preference task show surprising correlations to biological assays, and how they can be leveraged for few-shot hit maturation in CARs. This proof-of-concept presents a novel pathway for applying ML to immunotherapy and could generalise to other therapeutic modalities.","['preference optimization', 'protein language models', 'language models', 'immunotherapy']",,https://openreview.net/pdf?id=sajW6rHhyx,accept
https://openreview.net/forum?id=i1hekxcJP1,Correlational Lagrangian Schrodinger Bridge: Learning Dynamics with Population-Level Regularization,"['Yuning You', 'Ruida Zhou', 'Yang Shen']","Modeling population dynamics is a fundamental problem with broad scientific applications.
Motivated by real-world applications including biosystems with diverse populations, we consider a class of population dynamics modeling with two technical challenges: (i) dynamics to learn for individual particles are *heterogeneous* and (ii) available data to learn from are *not time-series* (i.e, each individual's state trajectory over time) but *cross-sectional* (i.e, the whole population's aggregated states without individuals matched over time).
To address the challenges, we introduce a novel computational framework dubbed **correlational Lagrangian Schr\""odinger bridge** (**CLSB**) that builds on optimal transport  to ""bridge"" cross-sectional data distributions. In contrast to prior methods regularizing all individuals' transport ""costs"" and then applying them to the population  *homogeneously*, CLSB directly regularizes *population* cost allowing for population *heterogeneity* and potentially improving model *generalizability*.
Specifically our contributions include 
**(1)** a novel population perspective of the transport cost and a new class of population regularizers capturing the temporal variations in multivariate relations, with the tractable formulation derived,
**(2)** three domain-informed instantiations of population regularizers on covariance, and **(3)** integration of population regularizers into data-driven generative models as constrained optimization and an approximate numerical solution, with further extension to conditional generative models.
Empirically, we demonstrate the superiority of CLSB in single-cell sequencing data analyses (including cell differentiation and drug-conditioned cell responses) and opinion depolarization.
Codes will be released upon acceptance.","['generative models', 'diffusion models']",,https://openreview.net/pdf?id=i1hekxcJP1,accept
https://openreview.net/forum?id=iExY38jh5R,Pharmacophore-based design by learning on voxel grids,"['Omar Mahmood', 'Pedro O. Pinheiro', 'Richard Bonneau', 'Saeed Saremi', 'Vishnu Sresht']","Ligand-based drug discovery (LBDD) relies on making use of known binders to a protein target to find structurally diverse molecules similarly likely to bind. This process typically involves a brute force search of the known binder (query) against a molecular library using some metric of molecular similarity. One popular approach overlays the pharmacophore-shape profile of the known binder to 3D conformations enumerated for each of the library molecules, computes overlaps, and picks a set of diverse library molecules with high overlaps. While this virtual screening workflow has had considerable success in hit diversification, scaffold hopping, and patent busting, it scales poorly with library sizes and restricts candidate generation to existing library compounds. Leveraging recent advances in voxel-based generative modelling, we propose a pharmacophore-based generative model and workflows that address the scaling and fecundity issues of conventional pharmacophore-based virtual screening. We introduce VoxCap, a voxel captioning method for generating SMILES strings from voxelised molecular representations. We propose two workflows as practical use cases as well as benchmarks for pharmacophore-based generation: de-novo design, in which we aim to generate new molecules with high pharmacophore-shape similarities to query molecules, and fast search, which aims to combine generative design with a cheap 2D substructure similarity search for efficient hit identification. Our results show that VoxCap significantly outperforms previous methods in generating diverse de-novo hits. When combined with our fast search workflow, VoxCap reduces computational time by orders of magnitude while returning hits for all query molecules, enabling the search of large libraries that are intractable to search by brute force.","['generative models', 'drug discovery', 'ligand-based drug discovery', 'pharmacophore', 'voxels', 'captioning']",,https://openreview.net/pdf?id=iExY38jh5R,accept
https://openreview.net/forum?id=W4d1a4odSc,GNNAS-Dock: Budget Aware Algorithm Selection with Graph Neural Networks for Molecular Docking,"['Yiliang Yuan', 'Mustafa Misir']","Molecular docking is a major element in drug discovery and design. It enables the prediction of ligand-protein interactions by simulating the binding of small molecules to proteins. Despite the availability of numerous docking algorithms, there is no single algorithm consistently outperforms the others across a diverse set of docking scenarios. This paper introduces GNNAS-Dock, a novel Graph Neural Network (GNN)-based automated algorithm selection system for molecular docking in blind docking situations. GNNs are accommodated to process the complex structural data of both ligands and proteins. They benefit from the inherent graph-like properties to predict the performance of various docking algorithms under different conditions. The present study pursues two main objectives: 1) predict the performance of each candidate docking algorithm, in terms of Root Mean Square Deviation (RMSD), thereby identifying the most accurate method for specific scenarios; and 2) choose the best computationally efficient docking algorithm for each docking case, aiming to reduce the time required for docking while maintaining high accuracy. We validate our approach on PDBBind 2020 refined set, which contains about 5,300 pairs of protein-ligand complexes. Our strategy is performed across a portfolio of 6 different state-of-the-art docking algorithms. To be specific, the candidate algorithms are DiffDock, DSDP, TankBind, GNINA, SMINA, Qvina-W. We additionally combine p2rank with GNINA, SMINA and Qvina-W for docking site prediction. Therefore, there are totally 9 different algorithms for selection. Our algorithm selection model achieves a mean RMSD of approximately 1.74 Ã…, significantly improving upon the top performing docking algorithm (DiffDock), which has a mean RMSD of 2.95 Ã…. Moreover, when making selection in consideration of computational efficiency, our model demonstrates a success rate of 79.73% in achieving an RMSD below the 2 Ã… threshold, with a mean RMSD value of 2.75 Ã… and an average processing time of about 29.05 seconds per instance. In contrast, the remaining docking algorithms like TankBind, though faster with a processing time of merely 0.03 seconds per instance, only achieve an RMSD below the 2 Ã… threshold in less than 60% of cases. These findings demonstrate the capability of GNN-based algorithm selection to significantly enhance docking performance while effectively reducing the computational time required, balancing efficiency with precision in molecular docking.","['molecular docking', 'automated algorithm selection', 'graph neural networks']",,https://openreview.net/pdf?id=W4d1a4odSc,accept
https://openreview.net/forum?id=gP3jnlfq51,Best Practices for Multi-Fidelity Bayesian Optimization in Materials and Molecular Research,"['Victor Sabanza Gil', 'Riccardo Barbano', 'Daniel Pacheco GutiÃ©rrez', 'Jeremy Scott Luterbacher', 'JosÃ© Miguel HernÃ¡ndez-Lobato', 'Philippe Schwaller', 'LoÃ¯c Roch']","Multi-fidelity Bayesian Optimization (MFBO) is a promising framework to speed
up materials and molecular discovery as sources of information of different accuracies are at hand at increasing cost. Despite its potential use in chemical tasks,
there is a lack of systematic evaluation of the many parameters playing a role
in MFBO. In this work, we provide guidelines and recommendations to decide
when to use MFBO in experimental settings. We investigate MFBO methods
applied to molecules and materials problems. First, we test two different families
of acquisition functions in two synthetic problems and study the effect of the
informativeness and cost of the approximate function. We use our implementation
and guidelines to benchmark three real discovery problems and compare them
against their single-fidelity counterparts. Our results may help guide future efforts
to implement MFBO as a routine tool in the chemical sciences.","['molecules', 'bayesian optimization', 'multi-fidelity']",,https://openreview.net/pdf?id=gP3jnlfq51,accept
https://openreview.net/forum?id=vZSkrgoI3l,FluxGAT: Integrating Flux Sampling with Graph Neural Networks for Unbiased Gene Essentiality Classification,"['Kieren Sharma', 'Zahraa S. Abdallah', 'Lucia Marucci']","Gene essentiality, the necessity of a specific gene for the survival of an organism, is crucial to our understanding of cellular processes and identifying drug targets. Experimental determination of gene essentiality requires large growth screens that are time-consuming and expensive, motivating the development of in silico approaches. Existing methods predominantly utilise flux balance analysis (FBA), a constraint-based optimisation algorithm; however, they are fundamentally limited by the necessity of a predefined cellular objective function. This requirement introduces an element of observer bias, as the objective function often reflects the researcherâ€™s assumptions rather than the cellâ€™s biological goals. Here, we present FluxGAT, a graph neural network (GNN) model capable of predicting gene essentiality directly from graphical representations of flux sampling data. Flux sampling removes the need for objective functions, thereby eliminating observer bias. FluxGAT leverages the unique strengths of GNNs in learning representations of complex relationships within metabolic reaction networks. The success of our approach in predicting experimentally determined gene essentiality, with almost double the sensitivity of FBA, explores the possibility of predicting cellular phenotypes in cases when objectives are less understood. Thus, we demonstrate a method for more general gene essentiality predictions across a broader spectrum of biological systems and environments.","['Graph Representation Learning', 'Flux Sampling', 'Gene Essentiality', 'Systems Biology']",,https://openreview.net/pdf?id=vZSkrgoI3l,accept
https://openreview.net/forum?id=2fuFbdgq2I,MF-LAL: Drug Compound Generation Using Multi-Fidelity Latent Space Active Learning,"['Peter Eckmann', 'Dongxia Wu', 'Germano Heinzelmann', 'Michael K Gilson', 'Rose Yu']","Current generative models for drug discovery primarily use molecular docking as an oracle to guide the generation of active compounds. However, such models are often not useful in practice because even compounds with high docking scores do not consistently show experimental activity. More accurate methods for activity prediction exist, such as molecular dynamics based binding free energy calculations, but they are too computationally expensive to use in a generative model. To address this challenge, we propose Multi-Fidelity Latent space Active Learning (MF-LAL), a generative modeling framework that integrates a set of oracles with varying cost-accuracy tradeoffs. Unlike previous approaches that separately learn the surrogate model and generative model, MF-LAL combines the generative and multi-fidelity surrogate models into a single framework, allowing for more accurate activity prediction and higher quality samples. We train MF-LAL with a novel active learning algorithm to further reduce computational cost. Our experiments on two disease-relevant proteins show that MF-LAL produces compounds with significantly better binding free energy scores than other single and multi-fidelity approaches. The code is available at https://github.com/Rose-STL-Lab/MF-LAL.","['drug discovery', 'multi-fidelity learning', 'generative models']",,https://openreview.net/pdf?id=2fuFbdgq2I,accept
https://openreview.net/forum?id=oZyIsX5LZD,Improving Structural Plausibility in 3D Molecule Generation via Property-Conditioned Training with Distorted Molecules,"['Lucy Vost', 'Vijil Chenthamarakshan', 'Payel Das', 'Charlotte Deane']","Traditional drug design methods are costly and time-consuming due to their reliance on trial-and-error processes. As a result, computational methods, including diffusion models, designed for molecule generation tasks have gained significant traction. Despite their potential, they have faced criticism for producing physically implausible outputs. We alleviate this problem by conditionally training a diffusion model capable of generating molecules of varying and controllable levels of structural plausibility. This is achieved by adding distorted molecules to training datasets, and then annotating each molecule with a label representing the extent of its distortion, and hence its quality. By training the model to distinguish between favourable and unfavourable molecular conformations alongside the standard molecule generation training process, we can selectively sample molecules from the high-quality region of learned space, resulting in improvements in the validity of generated molecules. In addition to the standard two datasets used by molecule generation methods (QM9 and GEOM), we also test our method on a druglike dataset derived from ZINC. We use our conditional method with EDM, the first E(3) equivariant diffusion model for molecule generation, as well as two further modelsâ€”a more recent diffusion model and a flow matching modelâ€”which were built off EDM. We demonstrate improvements in validity as assessed by RDKit parsability and the PoseBusters test suite; more broadly, though, our findings highlight the effectiveness of conditioning methods on low-quality data to improve the sampling of high-quality data.","['small molecule drugs', 'diffusion', '3D', 'conditioning']",,https://openreview.net/pdf?id=oZyIsX5LZD,accept
https://openreview.net/forum?id=bnvys5czbQ,Exploring Log-Likelihood Scores for Ranking Antibody Sequence Designs,"['Talip Ucar', 'Cedric Malherbe', 'Ferran Gonzalez']","Generative models trained on antibody sequences and structures have shown great potential in advancing machine learning-assisted antibody engineering and drug discovery. Current state-of-the-art models are primarily evaluated using two categories of in silico metrics: sequence-based metrics, such as amino acid recovery (AAR), and structure-based metrics, including root-mean-square deviation (RMSD), predicted alignment error (pAE), and interface predicted template modeling (ipTM). While metrics such as pAE and ipTM have been shown to be useful filters for experimental success, there is no evidence that they are suitable for ranking, particularly for antibody sequence designs. Furthermore, no reliable sequence-based metric for ranking has been established. In this work, using real-world experimental data from seven diverse datasets, we extensively benchmark a range of generative models, including LLM-style, diffusion-based, and graph-based models. We show that log-likelihood scores from these generative models correlate well with experimentally measured binding affinities, suggesting that log-likelihood can serve as a reliable metric for ranking antibody sequence designs. Additionally, we scale up one of the diffusion-based models by training it on a large and diverse synthetic dataset, significantly enhancing its ability to predict and score binding affinities. Our implementation is available at: https://github.com/AstraZeneca/DiffAbXL","['Generative models', 'Diffusion', 'LLM', 'graph', 'antibody', 'binding affinity', 'in silico metrics']",,https://openreview.net/pdf?id=bnvys5czbQ,accept
https://openreview.net/forum?id=EFBxt932b9,PepDoRA: A Unified Peptide Language Model via Weight-Decomposed Low-Rank Adaptation,"['Leyao Wang', 'Rishab Pulugurta', 'Pranay Vure', 'Yinuo Zhang', 'Aastha Pal', 'Pranam Chatterjee']","Peptide therapeutics, including macrocycles, peptide inhibitors, and bioactive linear peptides, play a crucial role in therapeutic development due to their unique physicochemical properties. However, predicting these properties remains challenging. While structure-based models primarily focus on local interactions, language models are capable of capturing global therapeutic properties of both modified and linear peptides. Protein language models like ESM-2, though effective for natural peptides, cannot however encode chemical modifications. Conversely, pre-trained chemical language models excel in representing small molecule properties but are not optimized for peptides. To bridge this gap, we introduce PepDoRA, a unified peptide representation model. Leveraging Weight-Decomposed Low-Rank Adaptation (DoRA), PepDoRA efficiently fine-tunes the ChemBERTa-77M-MLM on a masked language model objective to generate optimized embeddings for downstream property prediction tasks involving both modified and unmodified peptides. By tuning on a diverse and experimentally valid set of 100,000 modified, bioactive, and binding peptides, we show that PepDoRA embeddings capture functional properties of input peptides, enabling the accurate prediction of membrane permeability, non-fouling and hemolysis propensity, and via contrastive learning, target protein-specific binding. Overall, by providing a unified representation for chemically and biologically diverse peptides, PepDoRA serves as a versatile tool for function and activity prediction, facilitating the development of peptide therapeutics across a broad spectrum of applications.","['Peptides', 'PEFT', 'Chemical Language Models']",,https://openreview.net/pdf?id=EFBxt932b9,accept
https://openreview.net/forum?id=BuWVX4OxB8,A Deep Generative Model for the Design of Synthesizable Ionizable Lipids,"['Yuxuan Ou', 'Jingyi Zhao', 'Austin Tripp', 'Morteza Rasoulianboroujeni', 'JosÃ© Miguel HernÃ¡ndez-Lobato']","Lipid nanoparticles (LNPs) are vital in modern biomedicine, enabling the effective delivery of mRNA for vaccines and therapies by protecting it from rapid degradation. Among the components of LNPs, ionizable lipids play a key role in RNA protection and facilitate its delivery into the cytoplasm. However, designing ionizable lipids is complex. Deep generative models can accelerate this process and explore a larger candidate space compared to traditional methods. Due to the structural differences between lipids and small molecules, existing generative models used for small molecule generation are unsuitable for lipid generation. To address this, we developed a deep generative model specifically tailored for the discovery of ionizable lipids. Our model generates novel ionizable lipid structures and provides synthesis paths using synthetically accessible building blocks, addressing synthesizability. This advancement holds promise for streamlining the development of lipid-based delivery systems, potentially accelerating the deployment of new therapeutic agents, including mRNA vaccines and gene therapies.","['drug discovery', 'generative model', 'synthesizability', 'ionizable lipid', 'LNP']",,https://openreview.net/pdf?id=BuWVX4OxB8,accept
https://openreview.net/forum?id=fZNNHn6F2e,Video Representation Learning of Cardiac MRI for Genetic Discovery,"['Matt Sooknah', 'Sivaramakrishnan Sankarapandian', 'Ramprakash Srinivasan', 'Johannes Riegler', 'Jun Xu']","In recent years, many studies have utilized cardiac magnetic resonance imaging (cMRI) to define image-derived phenotypes (IDPs) relating to heart structure and function for genome-wide association studies (GWAS). These IDPs are traditionally defined manually from volume, strain, and geometric parameters. Here we introduce an unsupervised learning approach that extracts spatiotemporal representations from cMRI videos in a large human cohort of ~68,000 subjects from the UK BioBank. The resulting representations can be used to predict age and manually crafted IDPs accurately. We further use these representations to define IDPs to capture both known and potential novel genetic associations. Our work suggests that unsupervised learning can be used to extract rich, unbiased information from medical videos with applications to genetic discovery.","['representation learning', 'medical imaging', 'target discovery', 'masked autoencoders', 'cardiac MRI']",,https://openreview.net/pdf?id=fZNNHn6F2e,accept
https://openreview.net/forum?id=Kis8tVUeNi,Accurate and General DNA Representations Emerge from Genome Foundation Models at Scale,"['Caleb Ellington', 'Ning Sun', 'Nicholas Ho', 'Tianhua Tao', 'Sazan Mahbub', 'Dian Li', 'Yonghao Zhuang', 'Hongyi Wang', 'Eric P. Xing', 'Le Song']","Language models applied to protein sequences have become a panacea, enabling therapeutics development, materials engineering, and core biology research.
Despite the successes of protein language models, genome language models remain nascent.
Recent studies suggest the bottleneck is data volume or modeling context size, since long-range interactions are widely acknowledged but sparsely annotated.
However, it may be the case that even short DNA sequences are modeled poorly by existing approaches, and current models are unable to represent the wide array of functions encoded by DNA.
To study this, we develop AIDO.DNA,
a pretrained module for DNA representation in an AI-driven Digital Organism [1]. 
AIDO.DNA is a seven billion parameter encoder-only transformer trained on 10.6 billion nucleotides from a dataset of 796 species.
By scaling model size while maintaining a short context length of 4k nucleotides, AIDO.DNA shows substantial improvements across a breadth of supervised, generative, and zero-shot tasks relevant to functional genomics, synthetic biology, and drug development.
Notably, AIDO.DNA outperforms prior encoder-only architectures _without_ new data, suggesting that new scaling laws are needed to achieve compute-optimal DNA language models.
Models and code are available through ModelGenerator in https://github.com/genbio-ai/AIDO and on Hugging Face.","['Genomics', 'Foundation Models', 'Pretraining', 'Transfer Learning']",,https://openreview.net/pdf?id=Kis8tVUeNi,accept
https://openreview.net/forum?id=7aCRpxvu2N,Active learning for efficient discovery of optimal gene combinations in the combinatorial perturbation space,"['Jason Qin', 'Hans-Hermann Wessels', 'Carlos Fernandez-Granda', 'Yuhan Hao']","The advancement of novel combinatorial CRISPR screening technologies enables the identification of synergistic gene combinations on a large scale. This is crucial for developing novel and effective combination therapies, but the combinatorial space makes exhaustive experimentation infeasible. We introduce NAIAD, an active learning framework that efficiently discovers optimal gene pairs capable of driving cells toward desired cellular phenotypes. NAIAD leverages single-gene perturbation effects and adaptive gene embeddings that scale with the training data size, mitigating overfitting in small-sample learning while capturing complex gene interactions as more data is collected. Evaluated on four CRISPR combinatorial perturbation datasets totaling over 350,000 genetic interactions, NAIAD, trained on small datasets, outperforms existing models by up to 40\% relative to the second-best. NAIAD's recommendation system prioritizes gene pairs with the maximum predicted effects, resulting in the highest marginal gain in each AI-experiment round and accelerating discovery with fewer CRISPR experimental iterations. Our NAIAD framework (https://github.com/NeptuneBio/NAIAD) improves the identification of novel, effective gene combinations, enabling more efficient CRISPR library design and offering promising applications in genomics research and therapeutic development.","['active learning', 'combinatorial perturbation', 'AI-based CRISPR design']",,https://openreview.net/pdf?id=7aCRpxvu2N,accept
https://openreview.net/forum?id=Qn64CPMoGu,Antibody Library Design by Seeding Linear Programming with Inverse Folding and Protein Language Models,"['Conor F. Hayes', 'Andre R Goncalves', 'Steven Alan Magana-Zook', 'Ahmet Can Solak', 'Daniel faissol', 'Mikel Landajuela']","We propose a novel approach for antibody library design that combines deep learning and multi-objective linear programming with diversity constraints. Our method leverages recent advances in sequence and structure-based deep learning for protein engineering to predict the effects of mutations on antibody properties. These predictions are then used to seed a cascade of constrained integer linear programming problems, the solutions of which yield a diverse and high-performing antibody library. Operating in a cold-start setting, our approach creates designs without iterative feedback from wet laboratory experiments or computational simulations. We demonstrate the effectiveness of our method by designing antibody libraries for Trastuzumab in complex with the HER2 receptor, showing that it outperforms existing techniques in overall quality and diversity of the generated libraries.","['Antibody library design', 'deep learning', 'inverse folding', 'protein language models', 'multi-objective optimization', 'integer linear programming']",,https://openreview.net/pdf?id=Qn64CPMoGu,accept
https://openreview.net/forum?id=FyeZcS32wO,Understanding Protein-DNA Interactions by Paying Attention to Protein and Genomics Foundation Models,"['Dhruva Rajwade', 'Erica Wang', 'Aryan Satpathy', 'Alexander Brace', 'Hongyu Guo', 'Arvind Ramanathan', 'Shengchao Liu', 'Anima Anandkumar']","Protein-nucleic acid (NA) interactions are key in controlling gene regulation. There lies a strong motivation in understanding these interactions, with a goal of engineering these interactions to solve biological problems. Current methods to quantify protein-nucleic acids are mainly experimental and require much time and money. To mitigate this, Deep learning methods have recently been applied to predict Protein-DNA contacts. Although promising, these methods are computationally expensive and face challenges in accuracy. To address these challenges, we propose Seq2Contact, a novel method to predict the protein-NA binding at a single nucleotide (DNA) and single amino acid (Protein) level. Seq2Contact is built on protein and DNA foundation models to obtain nucleotide and amino acid-specific embeddings and then introduces a cross-attention module to obtain the binding contact maps. We employ a sequence-similarity-based clustering method to split the train-test data and empirically illustrate that Seq2Contact can achieve state-of-the-art performance, beating existing baselines by almost 20% (F1-Score) for Protein-NA binding prediction. Our method is computationally more efficient, with up to 80% less memory cost and more than 90% less inference time. Code is available at https://github.com/DhruvaRajwade/Seq2Contact","['Protein-DNA interactions', 'Cross-Attention', 'Binding map prediction', 'Finetuning', 'Protein language models', 'Genomics language models']",,https://openreview.net/pdf?id=FyeZcS32wO,accept
https://openreview.net/forum?id=jEDYKPDdwF,Latent Diffusion Models for Controllable RNA Sequence Generation,"['Kaixuan Huang', 'Yukang Yang', 'Kaidi Fu', 'Yanyi Chu', 'Le Cong', 'Mengdi Wang']","This work presents RNAdiffusion, a latent diffusion model for generating and optimizing discrete RNA sequences of variable lengths. RNA is a key intermediary between DNA and protein, exhibiting high sequence diversity and complex three-dimensional structures to support a wide range of functions. We utilize pretrained BERT-type models to encode raw RNA sequences into token-level, biologically meaningful representations. A Query Transformer is employed to compress such representations into a set of fixed-length latent vectors, with an autoregressive decoder trained to reconstruct RNA sequences from these latent variables. We then develop a continuous diffusion model within this latent space. To enable optimization, we integrate the gradients of reward modelsâ€”surrogates for RNA functional propertiesâ€”into the backward diffusion process, thereby generating RNAs with high reward scores. Empirical results confirm that RNAdiffusion generates non-coding RNAs that align with natural distributions across various biological metrics. Further, we fine-tune the diffusion model on mRNA 5â€™ untranslated regions (5â€™-UTRs) and optimize sequences for high translation efficiencies. Our guided diffusion model effectively generates diverse 5â€™-UTRs with high Mean Ribosome Loading (MRL) and Translation Efficiency (TE), outperforming baselines in balancing rewards and structural stability trade-off. Our findings hold potential for advancing RNA sequence-function research and therapeutic RNA design.","['latent diffusion model', 'RNA sequence generation']",,https://openreview.net/pdf?id=jEDYKPDdwF,accept
https://openreview.net/forum?id=qaB7EPfSUT,Deep Interactions for Multimodal Molecular Property Prediction,"['Patrick Soga', 'Zhenyu Lei', 'Camille L. Bilodeau', 'Jundong Li']","Multi-modal learning by means of leveraging both 2D graph and 3D point cloud information has become a prevalent method to improve model performance in molecular property prediction. However, many recent techniques focus on specific pre-training tasks such as contrastive learning, feature blending, and atom/subgraph masking in order to learn multi-modality even though design of model architecture is also impactful for both pre-training and downstream task performance. Relying on pre-training tasks to align 2D and 3D modalities lacks direct interaction which may be more effective in multimodal learning. In this work, we propose MolInteract, which takes a simple yet effective architecture-focused approach to multimodal molecule learning which addresses these challenges. MolInteract leverages an interaction layer for fusing 2D and 3D information and fostering cross-modal alignment, showing strong results using even the simplest pre-training methods such as predicting features of the 3D point cloud and 2D graph. MolInteract exceeds several current state-of-the-art multimodal pre-training techniques and architectures on various downstream 2D and 3D molecule property prediction benchmark tasks.","['small molecules', 'graph neural networks', 'AI for science']",,https://openreview.net/pdf?id=qaB7EPfSUT,accept
https://openreview.net/forum?id=bBgctzPkJr,Learning Molecular Representation in a Cell,"['Gang Liu', 'Srijit Seal', 'John Arevalo', 'Zhenwen Liang', 'Anne E Carpenter', 'Meng Jiang', 'Shantanu Singh']","Predicting drug effcacy and safety in vivo requires information on biological responses (e.g., cell morphology and gene expression) to small molecule perturbations. However, current molecular representation learning methods do not provide a comprehensive view of cell states under these perturbations and struggle to remove noise, hindering model generalization. We introduce the Information Alignment (InfoAlign) approach to learn molecular representations through the information bottleneck method in cells. We integrate molecules and cellular response data as nodes into a context graph, connecting them with weighted edges based on chemical, biological, and computational criteria. For each molecule in a training batch, InfoAlign optimizes the encoder's latent representation with a minimality objective to discard redundant structural information. A sufficiency objective decodes the representation to align with different feature spaces from the molecule's neighborhood in the context graph. We demonstrate that the proposed sufficiency objective for alignment is tighter than existing encoder-based contrastive methods. Empirically, we validate representations from InfoAlign in two downstream applications: molecular property prediction against up to 27 baseline methods across four datasets, plus zero-shot molecule-morphology matching.","['Molecular Representation Learning', 'Drug Discovery', 'Cell Morphology', 'Gene Expression']",,https://openreview.net/pdf?id=bBgctzPkJr,accept
https://openreview.net/forum?id=cuD5ZfMwMk,Representation Learning based Target Discovery from UKBB MRI data,"['Sivaramakrishnan Sankarapandian', 'Ramprakash Srinivasan', 'Matt Sooknah', 'Elena Sorokin', 'Jun Xu']","Medical imaging technologies such as MRI and CT scans offer valuable insights into a person's biological condition. Phenotypes derived from these images are essential for the discovery of novel drug targets. Traditional Genome-Wide Association Studies (GWAS) on imaging derived phenotypes (IDPs) require laborious manual feature annotation, extraction of disease-related phenotypes, and subsequent analysis of their associations with genetic variations. This approach has two main limitations: (1) manual voxel-level annotations are time consuming and subjective, particularly for intricate features; (2) these annotations are often limited to a handful of human-definable features, overlooking the wealth of information present in the scans. To address these limitations, we propose an alternative approach to derive phenotypes, which we term embedding-derived phenotypes (EDPs). Our approach consists of two steps. First, we train a self-supervised representation learning model to transform scans into latent embeddings, eliminating the need for manual annotations. Second, we convert these embeddings into disease-relevant phenotypes, preserving the information that may be lost in manually derived phenotypes. Although there are numerous self-supervised representation learning methods, it is not straightforward to transform the embeddings from these models into disease-relevant phenotypes. We present two simple methods that leverage binary labels like ICD-10 codes and demonstrate that the proposed methods identify more biologically meaningful genetic associations compared to using ICD-10 codes alone as binary traits or manually derived phenotypes.","['representation learning', 'GWAS', 'masked autoencoders', 'UKBB MRI', 'video MAE', 'phenotypes']",,https://openreview.net/pdf?id=cuD5ZfMwMk,accept
https://openreview.net/forum?id=hwNuRnytWg,Learning to refine domain knowledge for biological network inference,"['Peiwen Li', 'Menghua Wu']","Perturbation experiments allow biologists to discover causal relationships between variables of interest, but the sparsity and high dimensionality of these data pose significant challenges for causal structure learning algorithms.
Biological knowledge graphs can bootstrap the inference of causal structures in these situations, but since they compile vastly diverse information, they can bias predictions towards well-studied systems.
Alternatively, amortized causal structure learning algorithms encode inductive biases through data simulation and train supervised models to recapitulate these synthetic graphs.
However, realistically simulating biology is arguably even harder than understanding a specific system.
In this work, we take inspiration from both strategies and propose an amortized algorithm for refining domain knowledge, based on data observations.
On real and synthetic datasets, we show that our approach outperforms baselines in recovering ground truth causal graphs and identifying errors in the prior knowledge with limited interventional data.","['biological network inference', 'knowledge graphs', 'causal structure learning', 'perturbations']",,https://openreview.net/pdf?id=hwNuRnytWg,accept
https://openreview.net/forum?id=VF17uJFe74,Diverse Genomic Embedding Benchmark for functional evaluation across the tree of life.,"['Jacob West-Roberts', 'Joshua Kravitz', 'Nishant Jha', 'Andre Cornman', 'Yunha Hwang']","Biological foundation models hold significant promise for deciphering complex biological functions. However, evaluating their performance on functional tasks remains challenging due to the lack of standardized benchmarks encompassing diverse sequences and functions. Existing functional annotations are often scarce, biased, and susceptible to train-test leakage, hindering robust evaluation. Furthermore, biological functions manifest at multiple scales, from individual residues to large genomic segments. To address these limitations, we introduce the Diverse Genomic Embedding Benchmark (DGEB), inspired by natural language embedding benchmarks. DGEB comprises six embedding tasks across 18 expert curated datasets, spanning sequences from all domains of life and encompassing both nucleic acid and amino acid modalities. Notably, four datasets enable direct comparison between models trained on different modalities. Benchmarking protein and genomic language models (pLMs and gLMs) on DGEB reveals performance saturation with model scaling on numerous tasks, especially on those with underrepresented sequences (e.g. Archaea). This highlights the limitations of existing modeling objectives and training data distributions for capturing diverse biological functions. DGEB is available as an open-source package with a public leaderboard at https://github.com/TattaBio/DGEB.",['Benchmarks; Embeddings; Protein language models; Genomic language models'],,https://openreview.net/pdf?id=VF17uJFe74,accept
https://openreview.net/forum?id=0wrovwz9dG,"AlphaFold3, a secret sauce for predicting mutational effects on protein-protein interactions","['Wei Lu', 'Jixian Zhang', 'Jiahua Rao', 'Zhongyue Zhang', 'Shuangjia Zheng']","AlphaFold3 has set the new state-of-the-art in predicting protein-protein complex structures. However, the complete picture of biomolecular interactions cannot be fully captured by static structures alone. In the field of protein engineering and antibody discovery, the connection from structure to function is often mediated by binding energy. This work benchmarks AlphaFold3 against SKEMPI, a commonly used binding energy dataset. We demonstrate that AlphaFold3 learns unique information and synergizes with force field, profile-based, and other deep learning methods in predicting the mutational effects on protein-protein interactions. We hypothesize that AlphaFold3 captures a more global effect of mutations by learning a smoother energy landscape, but it lacks the modeling of full atomic details that are better addressed by force field methods, which possess a more rugged energy landscape. Integrating both approaches could be an interesting future direction.",['Protein protein interaction; binding affinity; protein structure'],,https://openreview.net/pdf?id=0wrovwz9dG,accept
https://openreview.net/forum?id=ZJkqmSgZeH,BindingGYM: A Large-Scale Mutational Dataset Toward Deciphering Protein-Protein Interactions,"['Wei Lu', 'Jixian Zhang', 'Ming Gu', 'Shuangjia Zheng']","Protein-protein interactions are crucial for drug discovery and understanding biological mechanisms. Despite significant advances in predicting the structures of protein complexes, led by AlphaFold3, determining the strength of these interactions accurately remains a challenge. Traditional low-throughput experimental methods do not generate sufficient data for comprehensive benchmarking or training deep learning models. Deep mutational scanning (DMS) experiments provide rich, high-throughput data; however, they are often used incompletely, neglecting to consider the binding partners, and on a per-study basis without assessing the generalization capabilities of fine-tuned models across different assays. To address these limitations, we collected over ten million raw DMS data points and refined them to half a million high-quality points from twenty-five assays, focusing on protein-protein interactions. We intentionally excluded non-PPI DMS data pertaining to intrinsic protein properties, such as fluorescence or catalytic activity. Our dataset meticulously pairs binding energies with the sequences and structures of all interacting partners using a comprehensive pipeline, recognizing that interactions inherently involve at least two proteins. This curated dataset serves as a foundation for benchmarking and training the next generation of deep learning models focused on protein-protein interactions, thereby opening the door to a plethora of high-impact applications including understanding cellular networks and advancing drug target discovery and development.","['Mutation effects prediction', 'Protein Structure', 'Protein Protein Interaction', 'Benchmarks']",,https://openreview.net/pdf?id=ZJkqmSgZeH,accept
https://openreview.net/forum?id=W94LmKVt2c,Improving Molecular Graph Generation with Flow Matching and Optimal Transport,"['Xiaoyang Hou', 'Tian Zhu', 'Milong Ren', 'Dongbo Bu', 'Xin Gao', 'Chunming Zhang', 'Shiwei Sun']","Generating molecular graphs is crucial in drug design and discovery but remains challenging due to the complex interdependencies between nodes and edges. While diffusion models have demonstrated their potentiality in molecular graph design, they often suffer from unstable training and inefficient sampling. To enhance generation performance and training stability, we propose GGFlow, a discrete flow matching generative model incorporating optimal transport for molecular graphs and it incorporates an edge-augmented graph transformer to enable the direct communications among chemical bounds. Additionally, GGFlow introduces a novel goal-guided generation framework to control the generative trajectory of our model, aiming to design novel molecular structures with the desired properties. GGFlow demonstrates superior performance on both unconditional and conditional molecule generation tasks, outperforming existing baselines and underscoring its effectiveness and potential for wider application.","['molecular graph generation', 'flow matching', 'optimal transport']",,https://openreview.net/pdf?id=W94LmKVt2c,accept
https://openreview.net/forum?id=vEAI5JHpLW,Generative Model for Synthesizing Ionizable Lipids: A Monte Carlo Tree Search Approach,"['Jingyi Zhao', 'Yuxuan Ou', 'Austin Tripp', 'Morteza Rasoulianboroujeni', 'JosÃ© Miguel HernÃ¡ndez-Lobato']","Ionizable lipids are essential in developing lipid nanoparticles (LNPs) for effective messenger RNA (mRNA)  delivery. While traditional methods for designing new ionizable lipids are typically time-consuming, deep generative models have emerged as a powerful solution, significantly accelerating the molecular discovery process. However, a practical challenge arises as the molecular structures generated can often be difficult or infeasible to synthesize. This project explores Monte Carlo tree search (MCTS)-based generative models for synthesizable ionizable lipids. Leveraging a synthetically accessible lipid building block dataset and two specialized predictors to guide the search through chemical space, we introduce a policy network guided MCTS generative model capable of producing new ionizable lipids with available synthesis pathways.","['generative AI', 'drug discovery', 'synthesis planning', 'Monte Carlo tree search', 'ionizable lipids']",,https://openreview.net/pdf?id=vEAI5JHpLW,accept
https://openreview.net/forum?id=LewtDFkAoi,Small-cohort GWAS discovery with AI over massive functional genomics knowledge graph,"['Kexin Huang', 'Tony Zeng', 'Soner Koc', 'Alexandra Pettet', 'Martin Jinye Zhang', 'Jure Leskovec']","Genome-Wide Association Studies (GWAS) links genetic markers with diseases and is the cornerstone for the development of effective therapeutics. However, for a long tail of many uncommon diseases, the small GWAS sample sizes limit detection power and hamper development of effective treatments. The recent substantial growth in the size of functional genomics data presents a fresh opportunity to tackle these challenges. Here, we introduce KGWAS, a novel geometric deep learning method that leverages a knowledge graph to integrate massive functional information about variants, genes, gene programs, and their interactions, assessing variant-disease associations. Unlike conventional GWAS, which treats variants independently, our approach recognizes that variants influence disease through complex cellular networks. Our realistic simulations show that KGWAS is well-calibrated and powerful in identifying disease variants. We applied KGWAS to 21 independent UK Biobank diseases/traits from small subsampled cohorts (N=1-10K), and KGWAS produced significantly more independent associations that were replicable in the full cohort (average N=374K), 22.0%-89.9% higher than state-of-the-art baselines. Next, we applied KGWAS to 554 less common UK Biobank diseases (N_case<5K) and identified 183 novel loci, 46.9\% higher than the original GWAS, including rs2155219 associated with ulcerative colitis potentially via regulating LRRC32 expression in CD4+ regulatory T cells, and rs73127651 associated with myasthenia gravis potentially via regulating PPHLN1 expression in brain cell types. Overall, KGWAS is a flexible and powerful AI model to integrate the growing functional genomics data to discover novel variants for small cohort diseases.","['human genetics', 'graph neural networks', 'functional genomics']",,https://openreview.net/pdf?id=LewtDFkAoi,accept
https://openreview.net/forum?id=zxyP6YXknv,Generalized Flow Matching for Transition Dynamics Modeling,"['Haibo Wang', 'Yuxuan Qiu', 'Yanze Wang', 'Rob Brekelmans', 'Yuanqi Du']","Simulating transition dynamics between metastable states is a fundamental challenge in dynamical systems and stochastic processes with wide real-world applications in understanding protein folding, chemical reactions and neural activities. However, the computational challenge often lies on sampling exponentially many paths in which only a small fraction ends in the target metastable state due to existence of high energy barriers. To amortize the cost, we propose a data-driven approach to warm-up the simulation by learning nonlinear interpolations from local dynamics. Specifically, we infer a potential energy function from local dynamics data. To find plausible paths between two metastable states, we formulate a generalized flow matching framework that learns a vector field to sample propable paths between the two marginal densities under the learned energy function. Furthermore, we iteratively refine the model by assigning importance weights to the sampled paths and buffering more likely paths for training. We validate the effectiveness of the proposed method to sample probable paths on both synthetic and real-world molecular systems.","['Generative Models', 'Flow Matching', 'Schrodinger Bridge', 'Transition Path Sampling']",,https://openreview.net/pdf?id=zxyP6YXknv,accept
https://openreview.net/forum?id=cve8laG181,Understanding the Sources of Performance in Deep Drug Response Models Reveals Insights and Improvements,"['Nikhil Branson', 'Pedro R. Cutillas', 'Conrad Bessant']","Anti-cancer drug response prediction (DRP) using cancer cell lines plays a vital role in stratified medicine and drug discovery.  Recently there has been a surge of new deep learning (DL) models for DRP that improve on the performance of their predecessors. However, different models use different input data types and neural network architectures making it hard to find the source of these improvements. Here we consider multiple published DRP models that report state-of-the-art performance in predicting continuous drug response values. These models take the chemical structures of drugs and omics profiles of cell lines as input. By experimenting with these models and comparing with our own simple benchmarks we show that no performance comes from drug features, instead, performance is due to the transcriptomics cell line profiles. Furthermore, we show that, depending on the testing type, much of the current reported performance is a property of the training target values. To address these limitations we create novel models (BinaryET and BinaryCB) that predict binary drug response values, guided by the hypothesis that this reduces the noise in the drug efficacy data. Thus, better aligning them with biochemistry that can be learnt from the input data.
BinaryCB leverages a chemical foundation model, while  BinaryET is trained from scratch using a transformer-type model. We show that these models learn useful chemical drug features, which is the first time this has been demonstrated for multiple DRP testing types to our knowledge. 
We further show binarising the drug response values is what causes the models to learn useful chemical drug features. We also show that BinaryET improves performance over BinaryCB, and over the published models that report state-of-the-art performance.","['drug response prediction', 'perturbation prediction', 'stratified medicine', 'AI for drug discovery', 'Transformers', 'GNNs']",,https://openreview.net/pdf?id=cve8laG181,accept
https://openreview.net/forum?id=SBdOZhHbNg,Effective Protein-Protein Interaction Exploration with PPIretrieval,"['Chenqing Hua', 'Connor W. Coley', 'Guy Wolf', 'Doina Precup', 'Shuangjia Zheng']","Protein-protein interactions (PPIs) are crucial in regulating numerous cellular functions, including signal transduction, transportation, and immune defense. As the accuracy of multi-chain protein complex structure prediction improves, the challenge has shifted towards effectively navigating the vast complex universe to identify potential PPIs. Herein, we propose PPIretrieval, a retrieval model for protein-protein interaction exploration, which leverages existing PPI data to effectively search for potential PPIs in an embedding space, capturing rich geometric and chemical information of protein surfaces. When provided with an unseen query protein with its associated binding site, PPIretrieval effectively identifies a potential binding partner along with its corresponding binding site in an embedding space, facilitating the formation of protein-protein complexes. Our codes are available on https://anonymous.4open.science/r/ppi_search-9E39.",['protein-protein interaction; protein retrieval model'],,https://openreview.net/pdf?id=SBdOZhHbNg,accept
https://openreview.net/forum?id=rYS4esEOgX,EnzymeFlow: Generating Reaction-specific Enzyme Catalytic Pockets through Flow Matching and Co-Evolutionary Dynamics,"['Chenqing Hua', 'Yong Liu', 'Dinghuai Zhang', 'Odin Zhang', 'Sitao Luan', 'Kevin K Yang', 'Guy Wolf', 'Doina Precup', 'Shuangjia Zheng']","Enzyme design is a critical area in biotechnology, with applications ranging from drug development to synthetic biology. Traditional methods for enzyme function prediction or protein binding pocket design often fall short in capturing the dynamic and complex nature of enzyme-substrate interactions, particularly in catalytic processes. To address the challenges, we introduce EnzymeFlow, a generative model that employs flow matching with hierarchical pre-training and enzyme-reaction co-evolution to generate catalytic pockets for specific substrates and catalytic reactions.
Additionally, we introduce a large-scale, experimentally validated dataset of enzyme-reaction pairs, specifically designed for the catalytic pocket generation task, comprising a total of $328,192$ pairs. By incorporating evolutionary dynamics and reaction-specific adaptations, EnzymeFlow becomes a powerful model for designing enzyme pockets, which is capable of catalyzing a wide range of biochemical reactions. Experiments on the new dataset demonstrate the model's effectiveness in designing high-quality, functional enzyme catalytic pockets, paving the way for advancements in enzyme engineering and synthetic biology. The EnzymeFlow code can be found at https://anonymous.4open.science/r/EnzymeFlow-7420.",['enzyme design; flow model; protein design; protein evolution'],,https://openreview.net/pdf?id=rYS4esEOgX,accept
https://openreview.net/forum?id=UJhqLXsb5y,Learning multi-cellular representations of single-cell transcriptomics data enables characterization of patient-level disease states,"['Tianyu Liu', 'Edward De Brouwer', 'Tony Kuo', 'Nathaniel Lee Diamant', 'Missarova Alsu', 'Minsheng Hao', 'Hanchen', 'Hector Corrada Bravo', 'Gabriele Scalia', 'Aviv Regev', 'Graham Heimberg']","Single-cell RNA-seq (scRNA-seq) has become a prominent tool for studying human biology and disease. The availability of massive scRNA-seq datasets and advanced machine learning techniques has recently driven the development of single-cell foundation models that provide informative and versatile cell representations based on expression profiles. However, to understand disease states, we need to consider entire tissue ecosystems, simultaneously considering many different interacting cells. Here, we tackle this challenge by generating patient-level representations derived from multi-cellular expression context measured with scRNA-seq of tissues. We develop PaSCient, a novel model that employs a multi-level representation learning paradigm and provides importance scores at the individual cell and gene levels for fine-grained analysis across multiple cell types and gene programs characteristic of a given disease. We apply PaSCient to learn a disease model across a large-scale scRNA-seq atlas of 24.3 million cells from over 5,000 patients. Comprehensive and rigorous benchmarking demonstrates the superiority of PaSCient in disease classification and its multiple downstream applications, including dimensionality reduction, gene/cell type prioritization, and patient subgroup discovery.","['Disease Stratification', 'Disease Modelling', 'Multi-Instance Learning', 'Attention', 'Single-Cell Transcriptomics', 'Zero-Shot Learning', 'Response Prediction']",,https://openreview.net/pdf?id=UJhqLXsb5y,accept
https://openreview.net/forum?id=SeslKuVb6z,MeMDLM: De Novo Membrane Protein Design with Masked Discrete Diffusion Protein Language Models,"['Shrey Goel', 'Vishrut Thoutam', 'Edgar Mariano Marroquin', 'Aaron Gokaslan', 'Arash Firouzbakht', 'Sophia Vincoff', 'Volodymyr Kuleshov', 'Huong T. Kratochvil', 'Pranam Chatterjee']","Masked Diffusion Language Models (MDLMs) have recently emerged as a strong class of generative models, paralleling state-of-the-art (SOTA) autoregressive (AR) performance across natural language modeling domains. While there have been advances in AR as well as both latent and discrete diffusion-based approaches for protein sequence design, masked diffusion language modeling with protein language models (pLMs) is unexplored. In this work, we introduce MeMDLM, an MDLM tailored for membrane protein design, harnessing the SOTA pLM ESM-2 to de novo generate realistic membrane proteins for downstream experimental applications. Our evaluations demonstrate that MeMDLM-generated proteins exceed AR-based methods by generating sequences with greater transmembrane (TM) character. We further apply our design framework to scaffold soluble and TM motifs, demonstrating that MeMDLM-reconstructed sequences achieve greater biological similarity to their original counterparts compared to SOTA inpainting methods. Finally, we show that MeMDLM captures physicochemical membrane protein properties with similar fidelity as SOTA pLMs, paving the way for experimental applications. In total, our pipeline motivates future exploration of MDLM-based pLMs for protein design.","['Membrane Protein Design', 'Discrete Diffusion']",,https://openreview.net/pdf?id=SeslKuVb6z,accept
https://openreview.net/forum?id=KNTyaCNG7Y,Applications of Modular Co-Design  for De Novo 3D Molecule Generation,"['Danny Reidenbach', 'Filipp Nikitin', 'Olexandr Isayev', 'Saee Gopal Paliwal']","De novo 3D molecule generation is a pivotal task in drug discovery. However, many recent geometric generative models struggle to produce high-quality 3D structures, even if they maintain 2D validity and topological stability. To tackle this issue and enhance the learning of effective molecular generation dynamics, we present Megalodonâ€“a family of simple and scalable transformer models. These models are enhanced with basic equivariant layers and trained using a joint continuous and discrete denoising co-design objective. We assess Megalodonâ€™s performance on established molecule generation benchmarks and introduce new 3D structure benchmarks that evaluate a modelâ€™s capability to generate realistic molecular structures, particularly focusing on energetics. We show that Megalodon achieves state-of-the-art results in 3D molecule generation, conditional structure generation, and structure energy benchmarks using diffusion and flow matching. Furthermore, we demonstrate that scaling Megalodon produces up to 49x more valid molecules at large sizes and 2-10x lower energy compared to the prior best generative models.","['diffusion', 'model scaling', 'molecule generation', 'flow matching']",,https://openreview.net/pdf?id=KNTyaCNG7Y,accept
https://openreview.net/forum?id=bm3aARS2Nw,Saturn: Sample-efficient Generative Molecular Design using Memory Manipulation,"['Jeff Guo', 'Philippe Schwaller']","Generative molecular design for drug discovery has very recently achieved a wave of experimental validation, with language-based backbones being the most common architectures employed. The most important factor for downstream success is whether an *in silico* oracle is well correlated with the desired end-point. To this end, current methods use cheaper proxy oracles with higher throughput before evaluating the most promising subset with high-fidelity oracles. The ability to *directly* optimize high-fidelity oracles would greatly enhance generative design and be expected to improve hit rates. However, current models are not efficient enough to consider such a prospect, exemplifying the sample efficiency problem. In this work, we introduce **Saturn**, which leverages the Augmented Memory algorithm and demonstrates the first application of the Mamba architecture for generative molecular design. We elucidate *how* experience replay with data augmentation improves sample efficiency and *how* Mamba synergistically exploits this mechanism. Saturn outperforms 22 models on multi-parameter optimization tasks relevant to drug discovery and may possess sufficient sample efficiency to consider the prospect of directly optimizing high-fidelity oracles.","['generative molecular design', 'small molecule', 'sample efficiency', 'drug discovery', 'reinforcement learning']",,https://openreview.net/pdf?id=bm3aARS2Nw,accept
https://openreview.net/forum?id=Ty0aKMV7Ow,Detection of RNA Editing Sites by GPT Fine-tuning,"['Zohar Rosenwasser', 'Erez Levanon', 'Michael Levitt', 'Gal Oren']","Accurately predicting RNA editing sites is crucial for leveraging endogenous base editing technologies for therapeutic applications. This study introduces a novel methodology leveraging advanced AI techniques, specifically OpenAI's GPT-3.5,  to predict both the occurrence and efficiency of RNA editing by base editors such as ADAR enzymes. By fine-tuning GPT models on extensive datasets of RNA sequences and secondary structures, we observe improvements in predictive accuracy, with our approach outperforming existing approaches. Our approach involves framing the problem in two distinct ways: as a generation problem, predicting new edited structures, and as a classification problem, determining if specific sites are edited. We also implement robust data augmentation strategies and threshold adjustments to optimize the model's performance. Our findings highlight the transformative potential of GPT in solving complex biological problems, providing a robust framework for future genetic interventions.

The sources of this work are available at our repository: https://github.com/Scientific-Computing-Lab/GPT_RNA_Editing_Detection","['RNA editing', 'A-to-I editing', 'ADAR enzymes', 'base editors', 'secondary structure', 'RNA sequence', 'RNA-binding proteins', 'machine learning', 'large language models', 'GPT-3.5', 'RNA editing prediction', 'data augmentation', 'threshold adjustment', 'ViennaRNA', 'biomedical AI', 'generative AI', 'classification', 'RNA editing efficiency', 'therapeutic RNA interventions', 'deep learning', 'secondary structure prediction']",,https://openreview.net/pdf?id=Ty0aKMV7Ow,accept
https://openreview.net/forum?id=TJdysxQ0CX,MolKD: Distilling Cross-Modal Knowledge in Chemical Reactions for Molecular Property Prediction,['Liang Zeng'],"How to effectively represent molecules is a long-standing challenge for molecular property prediction and drug discovery. 
This paper studies this problem and proposes to incorporate chemical domain knowledge, specifically related to chemical reactions, for learning effective molecular representations. 
However, the inherent cross-modality property between chemical reactions and molecules presents a significant challenge to address.
To this end, we introduce a novel method, namely MolKD, which Distills cross-modal Knowledge in chemical reactions to assist Molecular property prediction. 
Specifically, the reaction-to-molecule distillation model within MolKD transfers cross-modal knowledge from a pre-trained teacher network learning with one modality (i.e., reactions) into a student network learning with another modality (i.e., molecules).
Moreover, MolKD learns effective molecular representations by incorporating reaction yields to measure transformation efficiency of the reactant-product pair when pre-training on reactions. Extensive experiments demonstrate that MolKD significantly outperforms various competitive baseline models, e.g., 2.1\% absolute AUC-ROC gain on Tox21. Further investigations demonstrate that pre-trained molecular representations in MolKD can distinguish chemically reasonable molecular similarities, which enables molecular property prediction with high robustness and interpretability.","['Molecular Property Prediction', 'Chemical Reaction', 'Knowledge Distillation']",,https://openreview.net/pdf?id=TJdysxQ0CX,accept
https://openreview.net/forum?id=VlEC4llTP3,TaxDiff: Taxonomic-Guided Diffusion Model for Protein Sequence Generation,"['Zongying Lin', 'Li Hao', 'Liuzhenghao Lv', 'Yu Wang', 'Bin Lin', 'Junwu Zhang', 'Zijun Chen', 'Calvin Yu-Chian Chen', 'Li Yuan', 'Yonghong Tian']","Designing protein sequences with specific biological functions and structural stability is of paramount importance in both biology and chemistry. Generative models have demonstrated their potential for reliable protein design. However, previous models have been constrained by their inability to generate protein sequences in a controlled manner, a capability that is crucial for various biological applications. In this work, we propose TaxDiff, a taxonomic-guided diffusion model for controllable protein sequence generation that combines biological species information with the generative capabilities of diffusion models to generate structurally stable proteins within the sequence space. Specifically, taxonomic control information is inserted into each layer of the transformer block to achieve fine-grained control. The combination of global and local attention ensures the sequence consistency and structural foldability of taxonomic-specific proteins. Extensive experiments demonstrate that TaxDiff can consistently achieve better performance on multiple protein sequence generation benchmarks in both taxonomic-guided controllable generation and unconditional generation. Notably, the sequences generated by TaxDiff even surpass those produced by direct-structure-generation models in terms of confidence based on predicted structures and require only a quarter of the time of models based on the diffusion model.",['Protein sequence generation; Protein sequence design'],,https://openreview.net/pdf?id=VlEC4llTP3,accept
https://openreview.net/forum?id=GCvCSPbspW,Similarity-Quantized Relative Difference Learning for Improved Molecular Activity Prediction,"['Karina Zadorozhny', 'Kangway V. Chuang', 'Bharath Sathappan', 'Ewan Wallace', 'Vishnu Sresht', 'Colin A Grambow']","Accurate prediction of molecular activities is crucial for efficient drug discovery, yet remains challenging due to limited and noisy datasets. We introduce Similarity-Quantized Relative Learning (SQRL), a learning framework that reformulates molecular activity prediction as relative difference learning between structurally similar pairs of compounds. SQRL uses precomputed molecular similarities to enhance training of graph neural networks and other architectures, and significantly improves accuracy and generalization in low-data regimes common in drug discovery. We demonstrate its broad applicability and real-world potential through benchmarking on public datasets as well as proprietary industry data. Our findings demonstrate that leveraging similarity-aware relative differences provides an effective paradigm for molecular activity prediction.","['molecular activity prediction', 'drug discovery', 'graph neural networks', 'machine learning', 'low-data regimes', 'molecular similarity', 'relative difference learning', 'activity cliffs', 'benchmarking', 'chemical informatics', 'deep learning', 'medicinal chemistry']",,https://openreview.net/pdf?id=GCvCSPbspW,accept
https://openreview.net/forum?id=cJ8vtWIYTC,Cell ontology guided transcriptome foundation model,"['Xinyu Yuan', 'Zhihao Zhan', 'Zuobai Zhang', 'Manqi Zhou', 'Jianan Zhao', 'Boyu Han', 'Yue Li', 'Jian Tang']","Transcriptome foundation models (TFMs) hold great promises of deciphering the transcriptomic language that dictate diverse cell functions by self-supervised learning on large-scale single-cell gene expression data, and ultimately unraveling the complex mechanisms of human diseases. However, current TFMs treat cells as independent samples and ignore the taxonomic relationships between cell types, which are available in cell ontology graphs. We argue that effectively leveraging this ontology information during the TFM pre-training can improve learning biologically meaningful gene co-expression patterns while preserving TFM as a general purpose foundation model for downstream zero-shot and fine-tuning tasks. To this end, we present **s**ingle **c**ell, **Cell-o**ntology guided TFM (scCello). We introduce cell-type coherence loss and ontology alignment loss, which are minimized along with the masked gene expression prediction loss during the pre-training. The novel loss component guide scCello to learn the cell-type-specific representation and the structural relation between cell types from the cell ontology graph, respectively. We pre-trained scCello on 22 million cells from CellxGene database leveraging their cell-type labels mapped to the cell ontology graph from Open Biological and Biomedical Ontology Foundry. Our TFM demonstrates competitive generalization and transferability performance over the existing TFMs on biologically important tasks including identifying novel cell types of unseen cells, prediction of cell-type-specific marker genes, and cancer drug responses.","['Cell ontology graph', 'transcriptome foundation model', 'large-scale pre-training', 'cell representation learning', 'single cell RNA sequencing data']",,https://openreview.net/pdf?id=cJ8vtWIYTC,accept
https://openreview.net/forum?id=HI09Q9u8SS,Epitope Generation for Peptide-based Cancer Vaccine using Goal-directed Wasserstein Generative Adversarial Network with Gradient Penalty,"['Yen-Che Hsiao', 'Abhishek Dutta']","We introduce a novel immunogenicity goal-directed peptide sequence generator in a Wasserstein generative adversarial network (GAN) with gradient penalty. The GAN is trained using the bladder cancer epitope sequences that are predicted to bind with the human leukocyte antigen, HLA-A*0201, to trigger cytotoxic T-cell immune responses. The convolutional neural network-based generator is guided by an immunogenicity predictor from DeepImmuno-CNN and a critic to generate immunogenic epitopes for bladder cancer vaccines. The convolutional neural network-based immunogenicity predictor is trained with class I peptide human leukocyte antigen sequences from the immune epitope database to produce a continuous immunogenic score. We incorporated the trained immunogenicity predictor by training the generator with the predicted immunogenicity score of the generated peptide sequences. We showed our generator can produce more immunogenic peptides after adding the predictor and can produce peptides that are similar to the epitopes shown in bladder cancerous cells.","['Convolutional neural network', 'deep learning', 'generative adversarial network', 'immunogenicity', 'peptide-based cancer vaccine']",,https://openreview.net/pdf?id=HI09Q9u8SS,accept
https://openreview.net/forum?id=8AmP6pQwyP,LatentDE: Latent-based Directed Evolution accelerated by Gradient Ascent for Protein Sequence Design,"['Thanh V. T. Tran', 'Nhat Khang Ngo', 'Viet Thanh Duy Nguyen', 'Truong Son Hy']","Directed evolution has been the most effective method for protein engineering that optimizes biological functionalities through a resource-intensive process of screening or selecting among a vast range of mutations. To mitigate this extensive procedure, recent advancements in machine learning-guided methodologies center around the establishment of a surrogate sequence-function model. In this paper, we propose Latent-based Directed Evolution (LDE), an evolutionary algorithm designed to prioritize the exploration of high-fitness mutants in the latent space. At its core, LDE is a regularized variational autoencoder (VAE), harnessing the capabilities of the state-of-the-art Protein Language Model (pLM), ESM-2, to construct a meaningful latent space of sequences. From this encoded representation, we present a novel approach for efficient traversal on the fitness landscape, employing a combination of gradient-based methods and directed evolution. Experimental evaluations conducted on eight protein sequence design tasks demonstrate the superior performance of our proposed LDE over previous baseline algorithms. We public our code at https://github.com/HySonLab/LatentDE.","['Protein design', 'Directed Evolution', 'Latent-based optimization', 'Protein language model']",,https://openreview.net/pdf?id=8AmP6pQwyP,accept
https://openreview.net/forum?id=ya5alTUbAW,Improving Antibody Design with Force-Guided Sampling in Diffusion Models,"['Paulina KulytÄ—', 'Francisco Vargas', 'Simon V Mathis', 'Yu Guang Wang', 'JosÃ© Miguel HernÃ¡ndez-Lobato', 'Pietro Lio']","Antibodies, crucial for immune defense, primarily rely on complementarity-determining regions (CDRs) to bind and neutralize antigens, such as viruses. The design of these CDRs determines the antibody's affinity and specificity towards its target. Generative models, particularly denoising diffusion probabilistic models (DDPMs), have shown potential to advance the structure-based design of CDR regions. However, only a limited dataset of bound antibody-antigen structures is available, and generalization to out-of-distribution interfaces remains a challenge. Physics based force-fields, which approximate atomic interactions, offer a coarse but universal source of information to better mold designs to target interfaces. Integrating this foundational information into diffusion models is, therefore, highly desirable. Here, we propose a novel approach to enhance the sampling process of diffusion models by integrating force field energy-based feedback. Our model, DiffForce, employs forces to guide the diffusion sampling process, effectively blending the two distributions. Through extensive experiments, we demonstrate that our method guides the model to sample CDRs with lower energy, enhancing both the structure and sequence of the generated antibodies.","['diffusion models', 'antibody design']",,https://openreview.net/pdf?id=ya5alTUbAW,accept
https://openreview.net/forum?id=lrqHtZvSZW,DiffER: Categorical Diffusion Models for Chemical Retrosynthesis,"['Sean Current', 'Ziqi Chen', 'Xia Ning', 'srinivasan parthasarathy']","Methods for automatic chemical retrosynthesis have found recent success through the application of models traditionally built for natural language processing, primarily through transformer neural networks. These models have demonstrated significant ability to translate between the SMILES encodings of chemical products and reactants, but are constrained as a result of their autoregressive nature. We propose DiffER, an alternative template-free method for retrosynthesis prediction in the form of categorical diffusion, which allows the entire output SMILES sequence to be predicted in unison. We construct an ensemble of diffusion models which achieves state of the art performance for top-1 accuracy and competitive performance for top-3 and top-5 accuracy. We prove that \ours is a strong baseline for a new class of template-free model and is capable of learning a variety of synthetic techniques used in laboratory settings.","['retrosynthesis', 'chemistry', 'diffusion', 'categorical diffusion', 'ensemble']",,https://openreview.net/pdf?id=lrqHtZvSZW,accept
